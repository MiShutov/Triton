{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbe9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "# os.environ['TRITON_INTERPRET'] = '1'\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_cuda_autotune_config():\n",
    "    configs = []\n",
    "    for num_stages, num_warps in [\n",
    "        (2, 4),\n",
    "        (2, 8),\n",
    "        (3, 4),\n",
    "        (3, 8),\n",
    "        (4, 4),\n",
    "    ]:\n",
    "        for BLOCK_SIZE_M in [16, 32, 64, 128]:#, 128]:\n",
    "            for BLOCK_SIZE_N in [32, 64, 128]:#[64, 128]:\n",
    "                for BLOCK_SIZE_K in [16, 32]:#[16, 32]:\n",
    "                    configs.append(\n",
    "                        triton.Config(\n",
    "                            {\n",
    "                                \"GROUP_SIZE_M\" : 8,\n",
    "                                \"BLOCK_SIZE_M\" : BLOCK_SIZE_M,\n",
    "                                \"BLOCK_SIZE_N\" : BLOCK_SIZE_N,\n",
    "                                \"BLOCK_SIZE_K\" : BLOCK_SIZE_K,\n",
    "                            }, \n",
    "                            num_stages=num_stages, \n",
    "                            num_warps=num_warps\n",
    "                        ),\n",
    "                    )                        \n",
    "    return configs\n",
    "\n",
    "    return [triton.Config(\n",
    "                                {\n",
    "                                    \"GROUP_SIZE_M\" : 8,\n",
    "                                    \"BLOCK_SIZE_M\" : 64,\n",
    "                                    \"BLOCK_SIZE_N\" : 128,\n",
    "                                    \"BLOCK_SIZE_K\" : 16,\n",
    "                                },\n",
    "                                num_stages=3, \n",
    "                                num_warps=4\n",
    "                            )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2683f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=_get_cuda_autotune_config(),\n",
    "    # configs=[\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"GROUP_SIZE_M\" : 8,\n",
    "    #             \"BLOCK_SIZE_M\" : 64,\n",
    "    #             \"BLOCK_SIZE_N\" : 128,\n",
    "    #             \"BLOCK_SIZE_K\" : 16,\n",
    "    #         },\n",
    "    #         num_stages=2, \n",
    "    #         num_warps=4\n",
    "    #     )\n",
    "    # ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp16(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp16[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94af5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=_get_cuda_autotune_config(),\n",
    "    # configs=[\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"GROUP_SIZE_M\" : 8,\n",
    "    #             \"BLOCK_SIZE_M\" : 128,\n",
    "    #             \"BLOCK_SIZE_N\" : 128,\n",
    "    #             \"BLOCK_SIZE_K\" : 16,\n",
    "    #         },\n",
    "    #         num_stages=4, \n",
    "    #         num_warps=4\n",
    "    #     )\n",
    "    # ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_int4_fp16_scaled(\n",
    "        scales_ptr,\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    scales_ptrs = scales_ptr + pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        \n",
    "        scales = tl.load(scales_ptrs)\n",
    "        b = b.to(tl.float16) * scales[None, :]\n",
    "\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16_scaled(a, b, scales):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    \n",
    "    assert list(scales.shape) == [N,]\n",
    "    assert scales.dtype == torch.float16\n",
    "    \n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_int4_fp16_scaled[grid](\n",
    "        scales,\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0d095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=_get_cuda_autotune_config(),\n",
    "    # configs=[\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"GROUP_SIZE_M\" : 8,\n",
    "    #             \"BLOCK_SIZE_M\" : 64,\n",
    "    #             \"BLOCK_SIZE_N\" : 128,\n",
    "    #             \"BLOCK_SIZE_K\" : 16,\n",
    "    #         },\n",
    "    #         num_stages=2, \n",
    "    #         num_warps=4\n",
    "    #     )\n",
    "    # ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_int2_fp16(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 4) + tl.arange(0, BLOCK_SIZE_N // 4)) % (N // 4)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 4) * 2\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0x3\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x2\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int2_fp16(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 4\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_int2_fp16[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77e3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=_get_cuda_autotune_config(),\n",
    "    # configs=[\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"GROUP_SIZE_M\" : 8,\n",
    "    #             \"BLOCK_SIZE_M\" : 64,\n",
    "    #             \"BLOCK_SIZE_N\" : 128,\n",
    "    #             \"BLOCK_SIZE_K\" : 16,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4\n",
    "    #     )\n",
    "    # ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp16_bigpack(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)) % (N // 8)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 8) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16_bigpack(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int32\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 8\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp16_bigpack[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0b8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=_get_cuda_autotune_config(),\n",
    "    # configs=[\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"GROUP_SIZE_M\" : 8,\n",
    "    #             \"BLOCK_SIZE_M\" : 64,\n",
    "    #             \"BLOCK_SIZE_N\" : 64,\n",
    "    #             \"BLOCK_SIZE_K\" : 16,\n",
    "    #         },\n",
    "    #         num_stages=2, \n",
    "    #         num_warps=4\n",
    "    #     )\n",
    "    # ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp32(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float32)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator # .to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp32(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float32\n",
    "    assert b.dtype == torch.int8\n",
    "\n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp32[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int8_to_int4(bits):\n",
    "    v1 = (bits >> 4) & 0xF\n",
    "    v0 = bits & 0xF\n",
    "\n",
    "    w = torch.stack([v0, v1], dim=-1) - 0x8\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 2)\n",
    "\n",
    "\n",
    "def decode_int8_to_int2(bits):\n",
    "    v3 = (bits >> 6) & 0x3\n",
    "    v2 = (bits >> 4) & 0x3\n",
    "    v1 = (bits >> 2) & 0x3\n",
    "    v0 = bits & 0x3\n",
    "\n",
    "    w = torch.stack([v0, v1, v2, v3], dim=-1) - 0x2\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 4)\n",
    "\n",
    "\n",
    "def decode_int32_to_int4(bits):\n",
    "    v0 = bits & 0xF\n",
    "    v1 = (bits >> 4) & 0xF\n",
    "    v2 = (bits >> 8) & 0xF\n",
    "    v3 = (bits >> 12) & 0xF\n",
    "    v4 = (bits >> 16) & 0xF\n",
    "    v5 = (bits >> 20) & 0xF\n",
    "    v6 = (bits >> 24) & 0xF\n",
    "    v7 = (bits >> 28) & 0xF\n",
    "    \n",
    "    w = torch.stack([v0, v1, v2, v3, v4, v5, v6, v7], dim=-1) - 0x8\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 8)\n",
    "\n",
    "\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# M, N, K = 3 * (4096,)\n",
    "# M, N, K = 3 * (1024,)\n",
    "# M, N, K = 3 * (16,)\n",
    "\n",
    "# y = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\")\n",
    "# x_decompressed = decode_int8_to_int4(x_compressed)\n",
    "\n",
    "# o1 = torch.matmul(y, x_decompressed.to(torch.float16)\n",
    "#                   )\n",
    "# o2 = triton_matmul_int4_fp16(y, x_compressed)\n",
    "# print(matmul_kernel_fp16.best_config)\n",
    "# # assert torch.all(torch.isclose(o1, o2))\n",
    "\n",
    "# o3 = triton_matmul_int4_fp32(y.float(), x_compressed)\n",
    "# print(matmul_kernel_fp32.best_config)\n",
    "# # assert torch.all(torch.isclose(o1, o3))\n",
    "\n",
    "# x_compressed_32bit = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\")\n",
    "# x_decompressed = decode_int32_to_int4(x_compressed_32bit)\n",
    "\n",
    "# o3 = triton_matmul_int4_fp16_bigpack(y, x_compressed_32bit)\n",
    "# print(matmul_kernel_fp16_bigpack.best_config)\n",
    "\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 4), dtype=torch.int8, device=\"cuda\")\n",
    "# x_decompressed = decode_int8_to_int2(x_compressed)\n",
    "\n",
    "# o1 = torch.matmul(y, x_decompressed.to(torch.float16))\n",
    "# o2 = triton_matmul_int2_fp16(y, x_compressed)\n",
    "# print(matmul_kernel_int2_fp16.best_config)\n",
    "\n",
    "\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\")\n",
    "# scales = torch.abs(torch.randn(N, dtype=torch.float16, device=\"cuda\"))\n",
    "# o2 = triton_matmul_int4_fp16_scaled(y, x_compressed, scales)\n",
    "# print(matmul_kernel_int4_fp16_scaled.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import marlin\n",
    "DEV = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def gen_quant4(m, n, groupsize=-1):\n",
    "    maxq = 2 ** 4 - 1\n",
    "    w = torch.randn((m, n), dtype=torch.half, device=DEV)\n",
    "    if groupsize != -1:\n",
    "        w = w.reshape((-1, groupsize, n))\n",
    "        w = w.permute(1, 0, 2)\n",
    "        w = w.reshape((groupsize, -1))\n",
    "    s = torch.max(torch.abs(w), 0, keepdim=True)[0]\n",
    "    s *= 2 / maxq\n",
    "    w = torch.round(w / s).int()\n",
    "    w += (maxq + 1) // 2\n",
    "    w = torch.clamp(w, 0, maxq)\n",
    "    ref = (w - (maxq + 1) // 2).half() * s\n",
    "    if groupsize != -1:\n",
    "        def reshape(w):\n",
    "            w = w.reshape((groupsize, -1, n))\n",
    "            w = w.permute(1, 0, 2)\n",
    "            w = w.reshape((m, n)).contiguous()\n",
    "            return w\n",
    "        ref = reshape(ref)\n",
    "        w = reshape(w)\n",
    "    s = s.reshape((-1, n)).contiguous()\n",
    "    linear = nn.Linear(m, n)\n",
    "    linear.weight.data = ref.t()\n",
    "    # Workaround to test some special cases that are forbidden by the API\n",
    "    layer = marlin.Layer(256, 256, groupsize=groupsize)\n",
    "    if groupsize == -1:\n",
    "        groupsize = m\n",
    "    layer.k = m\n",
    "    layer.n = n\n",
    "    layer.groupsize = groupsize\n",
    "    layer.B = torch.empty((m // 16, n * 16 // 8), dtype=torch.int, device=DEV)\n",
    "    layer.s = torch.empty((m // groupsize, n), dtype=torch.half, device=DEV)\n",
    "    layer.pack(linear, s.t())\n",
    "    q = layer.B\n",
    "    s = layer.s\n",
    "    return ref, q, s\n",
    "\n",
    "\n",
    "def marline_matmul(A, B, scales):\n",
    "    M, K = A.shape\n",
    "    N = B.shape[-1] // 2\n",
    "\n",
    "    C = torch.zeros((M, N), dtype=torch.half, device=DEV)\n",
    "    workspace = torch.zeros(N // 128 * 16, device=DEV)\n",
    "\n",
    "    thread_k, thread_n = 128, 128\n",
    "    marlin.mul(A, B, C, scales, workspace, thread_k, thread_n, -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b8959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul-performance:\n",
      "        K     M       N  torch_fp16  int4_fp16  marline_w4a16\n",
      "0   512.0  16.0   512.0    1.024000   1.170286       0.684449\n",
      "1  1024.0  16.0  1024.0    2.520615   2.340571       2.496610\n",
      "2  2048.0  16.0  2048.0    2.526689   3.971879       7.108990\n",
      "3  4096.0  16.0  4096.0    3.666350   5.408516      13.443283\n",
      "4  8192.0  16.0  8192.0    4.843307   6.969453      17.667201\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BS = 16\n",
    "sizes = [512, 1024, 2048, 4096, 8192]\n",
    "configs = []\n",
    "configs.append(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"K\", \"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        # x_vals=[(BS * size, size, size) for size in sizes],\n",
    "        x_vals=[(size, BS, size) for size in sizes],\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "        line_vals=[\n",
    "            \"torch_fp16\", \n",
    "            # \"triton_int2_fp16\",\n",
    "            \"triton_int4_fp16\",\n",
    "            # \"triton_int4_fp16_scaled\",\n",
    "            # \"triton_int4_fp16_bigpack\",\n",
    "            \"marline_w4a16\",\n",
    "            ],  # Label name for the lines\n",
    "        line_names=[\n",
    "            \"torch_fp16\", \n",
    "            # \"int2_fp16\",\n",
    "            \"int4_fp16\", \n",
    "            # \"int4_fp16_scaled\", \n",
    "            # \"int4_fp16_bigpack\",\n",
    "            \"marline_w4a16\",\n",
    "            ],  # Line styles\n",
    "        #styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        xlabel=\"Matrix size\",\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, K, N, provider):\n",
    "    # M, K = K, M\n",
    "    \n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\").contiguous() / (M * K)\n",
    "    \n",
    "    x_compressed_int2 = torch.randint(-128, 128, (K, N // 4), dtype=torch.int8, device=\"cuda\").contiguous()\n",
    "    x_compressed_int4 = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\").contiguous()\n",
    "    x_compressed_int4_bigpack = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\").contiguous()\n",
    "    x_decompressed_fp16 = decode_int8_to_int4(x_compressed_int4).reshape(K, N).to(torch.float16).contiguous()\n",
    "    scales = torch.abs(torch.randn(N, dtype=torch.float16, device=\"cuda\"))\n",
    "\n",
    "    _, x_compressed_marline, scales_marline = gen_quant4(K, N, groupsize=128)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(y_fp16, x_decompressed_fp16), quantiles=quantiles)\n",
    "    if provider == \"triton_int2_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int2_fp16(y_fp16, x_compressed_int2), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16(y_fp16, x_compressed_int4), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16_scaled\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16_scaled(y_fp16, x_compressed_int4, scales), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16_bigpack\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16_bigpack(y_fp16, x_compressed_int4_bigpack), quantiles=quantiles)\n",
    "    if provider == \"marline_w4a16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: marline_matmul(y_fp16, x_compressed_marline, scales_marline), quantiles=quantiles)\n",
    "    \n",
    "\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(show_plots=False, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa2f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUP_SIZE_M: 8, BLOCK_SIZE_M: 16, BLOCK_SIZE_N: 64, BLOCK_SIZE_K: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n"
     ]
    }
   ],
   "source": [
    "print(matmul_kernel_fp16.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a455c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
