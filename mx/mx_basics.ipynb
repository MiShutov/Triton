{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from triton.tools.tensor_descriptor import TensorDescriptor\n",
    "from triton.tools.mxfp import MXFP4Tensor, MXScaleTensor\n",
    "\n",
    "from kernel_round_fp4 import round_to_fp4, quantize_to_fp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e8818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(128 ,128)\n",
    "\n",
    "\n",
    "print(x.is_contiguous())\n",
    "print(x.T.is_contiguous())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP8_MAX = torch.finfo(torch.float8_e4m3fn).max\n",
    "FP4_MAX = 6.0\n",
    "NVFP4_GROUP_SIZE = 16\n",
    "\n",
    "\n",
    "def group_tensor(x, gs, col_wise=False):\n",
    "    assert len(x.shape)==2\n",
    "    if col_wise:\n",
    "        return x.T.reshape(x.shape[1], x.shape[0] // gs, gs)\n",
    "    return x.reshape(x.shape[0], x.shape[1] // gs, gs)\n",
    "\n",
    "\n",
    "torch.compile()\n",
    "def get_nvfp_scales(x, col_wise):\n",
    "    x_shape = x.shape\n",
    "\n",
    "    s_enc = (FP8_MAX * FP4_MAX) / x.abs().max().float()\n",
    "    s_dec = 1 / s_enc\n",
    "\n",
    "    x = group_tensor(x, NVFP4_GROUP_SIZE, col_wise=col_wise)\n",
    "\n",
    "    s_dec_b = x.abs().amax(dim=-1) / FP4_MAX\n",
    "    s_dec_b_fp8 = (s_dec_b * s_enc).to(torch.float8_e4m3fn)\n",
    "\n",
    "    s_enc_b = 1 / (s_dec_b_fp8.float() * s_dec)\n",
    "\n",
    "    x = (x * s_enc_b.unsqueeze(-1)).reshape(x_shape)\n",
    "    return x_scaled, \n",
    "\n",
    "\n",
    "def quantize_NVFP4(x, col_wise=False):\n",
    "    x_shape = x.shape\n",
    "\n",
    "    s_enc = (FP8_MAX * FP4_MAX) / x.abs().max().float()\n",
    "    s_dec = 1 / s_enc\n",
    "\n",
    "    x = group_tensor(x, NVFP4_GROUP_SIZE, col_wise=col_wise)\n",
    "\n",
    "    s_dec_b = x.abs().amax(dim=-1) / FP4_MAX\n",
    "    s_dec_b_fp8 = (s_dec_b * s_enc).to(torch.float8_e4m3fn)\n",
    "\n",
    "    s_enc_b = 1 / (s_dec_b_fp8.float() * s_dec)\n",
    "\n",
    "    x = (x * s_enc_b.unsqueeze(-1)).reshape(x_shape)\n",
    "\n",
    "    x_q = quantize_to_fp4(x)\n",
    "\n",
    "    return x_q, s_dec_b_fp8, s_dec\n",
    "\n",
    "\n",
    "def reconstruct_weight(x_packed, group_scales, tensor_scale, col_wise=False):\n",
    "    x_scaled = MXFP4Tensor(size=(x_packed.shape[0], x_packed.shape[1] * 2), device=x_packed.device)\n",
    "    x_scaled.data = x_scaled.unpack_packed_tensor(x_packed, dim=1, original_shape=x_scaled.size)\n",
    "    x_scaled_grouped = group_tensor(x_scaled.to(torch.float32), NVFP4_GROUP_SIZE, col_wise=False)\n",
    "    x_unpacked = (x_scaled_grouped * group_scales.float().unsqueeze(-1) * tensor_scale).reshape(x_packed.shape[0], x_packed.shape[1] * 2)\n",
    "    return x_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fc4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn(4096, 4096, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4694105",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled_packed, s_dec_b_fp8, s_dec = quantize_NVFP4(x, col_wise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd530fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "x_reco = reconstruct_weight(x_scaled_packed, s_dec_b_fp8, s_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_block_scaled(M, N, K, block_scale_type=\"nvfp4\", compute_reference=False):\n",
    "    BLOCK_M = 128\n",
    "    BLOCK_N = 256\n",
    "    BLOCK_K = 256 if \"fp4\" in block_scale_type else 128\n",
    "    VEC_SIZE = 16 if block_scale_type == \"nvfp4\" else 32\n",
    "    assert block_scale_type in [\"nvfp4\", \"mxfp4\", \"mxfp8\", \"mixed\"], f\"Invalid block scale type: {block_scale_type}\"\n",
    "    ELEM_PER_BYTE_A = 2 if \"fp4\" in block_scale_type else 1\n",
    "    ELEM_PER_BYTE_B = 1 if block_scale_type == \"mxfp8\" else 2\n",
    "\n",
    "    device = \"cuda\"\n",
    "    a_ref = MXFP4Tensor(size=(M, K), device=device).random()\n",
    "    # Similar to Hopper's wgmma symmetric fp8 instruction, the RHS is expected\n",
    "    # to be in col-major layout for Blackwell's tcgen05.mma when using fp4 operands.\n",
    "    # To conform to the expected semantics of tl.dot_scaled, (M, K) x (K, N),\n",
    "    # the data is generated in col-major layout, packed along K for fp4, and then\n",
    "    # logically transposed. Note that if one operand is of fp8 precision, unlike Hopper,\n",
    "    # Blackwell supports both row-major and col-major layouts for the RHS matrix.\n",
    "    # For the mixed-precision case, the fp4 RHS can be either in row or col-major layout.\n",
    "    # But for performance reason, it is recommended to use col-major layout. If TMA is used\n",
    "    # for the fp4 RHS operand load in mixed-precision dot, as in this tutorial, it must be\n",
    "    # in col-major layout.\n",
    "    b_ref = MXFP4Tensor(size=(N, K), device=device).random()\n",
    "    if block_scale_type in [\"mxfp8\", \"mixed\"]:\n",
    "        a_ref = a_ref.to(torch.float32)\n",
    "        a = a_ref.to(torch.float8_e4m3fn)\n",
    "    else:\n",
    "        # Pack two fp4 elements per byte along K\n",
    "        a = a_ref.to_packed_tensor(dim=1)\n",
    "\n",
    "    if block_scale_type == \"mxfp8\":\n",
    "        b_ref = b_ref.to(torch.float32)\n",
    "        b = b_ref.to(torch.float8_e4m3fn)\n",
    "    else:\n",
    "        b = b_ref.to_packed_tensor(dim=1)\n",
    "\n",
    "    b_ref = b_ref.to(torch.float32).T\n",
    "\n",
    "    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_M, BLOCK_K // ELEM_PER_BYTE_A])\n",
    "    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_N, BLOCK_K // ELEM_PER_BYTE_B])\n",
    "\n",
    "    a_scale_shape = [M // 128, K // VEC_SIZE // 4, 32, 16]\n",
    "    b_scale_shape = [N // 128, K // VEC_SIZE // 4, 32, 16]\n",
    "    epsilon = 1e-8\n",
    "    a_scale = torch.rand(a_scale_shape, device=device) + epsilon\n",
    "    b_scale = torch.rand(b_scale_shape, device=device) + epsilon\n",
    "    if block_scale_type == \"nvfp4\":\n",
    "        a_scale = a_scale.to(torch.float8_e4m3fn)\n",
    "        b_scale = b_scale.to(torch.float8_e4m3fn)\n",
    "        a_scale_ref = a_scale\n",
    "        b_scale_ref = b_scale\n",
    "    elif block_scale_type in [\"mxfp4\", \"mxfp8\", \"mixed\"]:\n",
    "        a_scale_ref = MXScaleTensor(a_scale)\n",
    "        b_scale_ref = MXScaleTensor(b_scale)\n",
    "        a_scale = a_scale_ref.data\n",
    "        b_scale = b_scale_ref.data\n",
    "\n",
    "    rep_m = BLOCK_M // 128\n",
    "    rep_n = BLOCK_N // 128\n",
    "    rep_k = BLOCK_K // VEC_SIZE // 4\n",
    "\n",
    "    # Use 5D TMA descriptor [1, rep_m, rep_k, 2, 256] with uint8 elements.\n",
    "    # With 256 elements we better utilize the L2 and don't require the TMA\n",
    "    # engine to emit many small messages (16B) messages as with 32x16xu8.\n",
    "    a_scale_block_shape = [1, rep_m, rep_k, 2, 256]\n",
    "    b_scale_block_shape = [1, rep_n, rep_k, 2, 256]\n",
    "    a_scale = a_scale.reshape(1, a_scale_shape[0], a_scale.shape[1], 2, 256)\n",
    "    b_scale = b_scale.reshape(1, b_scale_shape[0], b_scale.shape[1], 2, 256)\n",
    "    a_scale_desc = TensorDescriptor.from_tensor(a_scale, block_shape=a_scale_block_shape)\n",
    "    b_scale_desc = TensorDescriptor.from_tensor(b_scale, block_shape=b_scale_block_shape)\n",
    "\n",
    "    reference = None\n",
    "    if compute_reference:\n",
    "        a_scale_ref = a_scale_ref.to(torch.float32)\n",
    "        b_scale_ref = b_scale_ref.to(torch.float32)\n",
    "\n",
    "        def unpack_scale(packed):\n",
    "            packed = packed.reshape(*packed.shape[:-2], 32, 4, 4)\n",
    "            num_chunk_m, num_chunk_k, _, _, _ = packed.shape\n",
    "            return packed.permute(0, 3, 2, 1, 4).reshape(num_chunk_m * 128, num_chunk_k * 4).contiguous()\n",
    "\n",
    "        a_scale_ref = unpack_scale(a_scale_ref).repeat_interleave(VEC_SIZE, dim=1)[:M, :K]\n",
    "        b_scale_ref = unpack_scale(b_scale_ref).repeat_interleave(VEC_SIZE, dim=1).T.contiguous()[:K, :N]\n",
    "        \n",
    "        # print(a_ref.to(torch.float32).shape, a_scale_ref.shape, b_ref.shape, b_scale_ref.shape)\n",
    "        \n",
    "        reference = torch.matmul(a_ref.to(torch.float32) * a_scale_ref, b_ref * b_scale_ref)\n",
    "\n",
    "    configs = {\n",
    "        \"BLOCK_SIZE_M\": BLOCK_M,\n",
    "        \"BLOCK_SIZE_N\": BLOCK_N,\n",
    "        \"BLOCK_SIZE_K\": BLOCK_K,\n",
    "        \"num_stages\": 4,\n",
    "        \"ELEM_PER_BYTE_A\": ELEM_PER_BYTE_A,\n",
    "        \"ELEM_PER_BYTE_B\": ELEM_PER_BYTE_B,\n",
    "        \"VEC_SIZE\": VEC_SIZE,\n",
    "    }\n",
    "    return a_desc, a_scale_desc, b_desc, b_scale_desc, rep_m, rep_n, rep_k, configs, reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbb9ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m M, N, K = (\u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)\n\u001b[32m      2\u001b[39m block_scale_type=\u001b[33m\"\u001b[39m\u001b[33mnvfp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m a_desc, a_scale, b_desc, b_scale, rep_m, rep_n, rep_k, configs, reference = \u001b[43minitialize_block_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_scale_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36minitialize_block_scaled\u001b[39m\u001b[34m(M, N, K, block_scale_type, compute_reference)\u001b[39m\n\u001b[32m      8\u001b[39m ELEM_PER_BYTE_B = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block_scale_type == \u001b[33m\"\u001b[39m\u001b[33mmxfp8\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m2\u001b[39m\n\u001b[32m     10\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m a_ref = \u001b[43mMXFP4Tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Similar to Hopper's wgmma symmetric fp8 instruction, the RHS is expected\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# to be in col-major layout for Blackwell's tcgen05.mma when using fp4 operands.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# To conform to the expected semantics of tl.dot_scaled, (M, K) x (K, N),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# for the fp4 RHS operand load in mixed-precision dot, as in this tutorial, it must be\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# in col-major layout.\u001b[39;00m\n\u001b[32m     22\u001b[39m b_ref = MXFP4Tensor(size=(N, K), device=device).random()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/triton_env/lib/python3.13/site-packages/triton/tools/mxfp.py:36\u001b[39m, in \u001b[36mMXFP4Tensor.random\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrandom\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     S = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     E = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, size=\u001b[38;5;28mself\u001b[39m.size, dtype=torch.uint8, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     38\u001b[39m     M = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, size=\u001b[38;5;28mself\u001b[39m.size, dtype=torch.uint8, device=\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/triton_env/lib/python3.13/site-packages/torch/cuda/__init__.py:412\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    411\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    416\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "M, N, K = (128, 128, 128)\n",
    "block_scale_type=\"nvfp4\"\n",
    "a_desc, a_scale, b_desc, b_scale, rep_m, rep_n, rep_k, configs, reference = initialize_block_scaled(M, N, K, block_scale_type, compute_reference=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eed5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDescriptor(base=tensor([[246, 249, 160,  ..., 178, 137, 155],\n",
       "        [150, 213, 249,  ..., 185, 109,  66],\n",
       "        [ 24,  59,  92,  ..., 225,   3, 220],\n",
       "        ...,\n",
       "        [225,  54,  76,  ..., 103, 222, 192],\n",
       "        [123, 104, 202,  ..., 150, 108, 205],\n",
       "        [188,  42, 221,  ...,  25, 233, 155]], device='cuda:0',\n",
       "       dtype=torch.uint8), shape=torch.Size([128, 64]), strides=(64, 1), block_shape=[128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c89293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  9,  6,  ..., 14,  6,  3],\n",
       "         [12,  9, 14,  ..., 12,  3, 14],\n",
       "         [ 5, 11,  8,  ..., 13,  5, 10],\n",
       "         ...,\n",
       "         [ 9,  9,  9,  ..., 15,  0,  7],\n",
       "         [ 5,  6,  1,  ...,  0, 10,  7],\n",
       "         [ 9,  1, 12,  ...,  8,  5,  5]], device='cuda:0', dtype=torch.uint8),\n",
       " tensor([[144,  86,  30,  ...,  48, 227,  54],\n",
       "         [156, 142, 221,  ..., 228, 199, 227],\n",
       "         [181,  72,  57,  ..., 192, 216, 165],\n",
       "         ...,\n",
       "         [153, 153, 235,  ...,  34, 254, 112],\n",
       "         [101,  65,  27,  ..., 149,   4, 122],\n",
       "         [ 25, 220,  52,  ..., 115, 135,  85]], device='cuda:0',\n",
       "        dtype=torch.uint8),\n",
       " tensor([[ 0.0000, -0.5000,  4.0000,  ..., -4.0000,  4.0000,  1.5000],\n",
       "         [-2.0000, -0.5000, -4.0000,  ..., -2.0000,  1.5000, -4.0000],\n",
       "         [ 3.0000, -1.5000, -0.0000,  ..., -3.0000,  3.0000, -1.0000],\n",
       "         ...,\n",
       "         [-0.5000, -0.5000, -0.5000,  ..., -6.0000,  0.0000,  6.0000],\n",
       "         [ 3.0000,  4.0000,  0.5000,  ...,  0.0000, -1.0000,  6.0000],\n",
       "         [-0.5000,  0.5000, -2.0000,  ..., -0.0000,  3.0000,  3.0000]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ref = MXFP4Tensor(size=(M, K), device=\"cuda\").random()\n",
    "a_ref.data, a_ref.to_packed_tensor(dim=1), a_ref.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fp = torch.randn(128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81846104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  9., 10.,  ..., 10., 10.,  0.],\n",
       "        [ 3.,  8.,  9.,  ...,  9.,  9.,  2.],\n",
       "        [ 1.,  3.,  2.,  ...,  1.,  9.,  3.],\n",
       "        ...,\n",
       "        [12.,  2.,  2.,  ..., 11.,  1., 10.],\n",
       "        [ 1., 10.,  3.,  ...,  9.,  9., 10.],\n",
       "        [11.,  9.,  2.,  ...,  8., 11.,  9.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ref._from_float(x_fp).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorDescriptor(base=tensor([[[[[0.2344, 0.8125, 0.6875,  ..., 1.0000, 0.3750, 0.4062],\n",
       "            [0.0391, 0.5000, 0.9375,  ..., 0.4375, 0.5625, 0.1406]],\n",
       " \n",
       "           [[1.0000, 0.7500, 0.0625,  ..., 0.6875, 0.3125, 0.5625],\n",
       "            [0.4062, 0.5625, 0.2188,  ..., 0.5625, 0.2188, 0.1719]]]]],\n",
       "        device='cuda:0', dtype=torch.float8_e4m3fn), shape=torch.Size([1, 1, 2, 2, 256]), strides=(1024, 1024, 512, 256, 1), block_shape=[1, 1, 4, 2, 256]),\n",
       " tensor([[[[[0.2344, 0.8125, 0.6875,  ..., 1.0000, 0.3750, 0.4062],\n",
       "            [0.0391, 0.5000, 0.9375,  ..., 0.4375, 0.5625, 0.1406]],\n",
       " \n",
       "           [[1.0000, 0.7500, 0.0625,  ..., 0.6875, 0.3125, 0.5625],\n",
       "            [0.4062, 0.5625, 0.2188,  ..., 0.5625, 0.2188, 0.1719]]]]],\n",
       "        device='cuda:0', dtype=torch.float8_e4m3fn))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_scale, a_scale.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4188f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
