{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbe9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "# os.environ['TRITON_INTERPRET'] = '1'\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727feb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_cuda_autotune_config():\n",
    "    configs = []\n",
    "    for num_warps, num_stages in [\n",
    "        (2, 4),\n",
    "        (2, 8),\n",
    "        (3, 4),\n",
    "        (3, 8),\n",
    "        (4, 4),\n",
    "    ]:\n",
    "        for BLOCK_SIZE_M in [32, 64, 128]:#, 128]:\n",
    "            for BLOCK_SIZE_N in [32, 64, 128]:#[64, 128]:\n",
    "                for BLOCK_SIZE_K in [16, 32]:#[16, 32]:\n",
    "                    configs.append(\n",
    "                        triton.Config(\n",
    "                            {\n",
    "                                \"GROUP_SIZE_M\" : 8,\n",
    "                                \"BLOCK_SIZE_M\" : BLOCK_SIZE_M,\n",
    "                                \"BLOCK_SIZE_N\" : BLOCK_SIZE_N,\n",
    "                                \"BLOCK_SIZE_K\" : BLOCK_SIZE_K,\n",
    "                            }, \n",
    "                            num_stages=num_stages, \n",
    "                            num_warps=num_warps\n",
    "                        ),\n",
    "                    )                        \n",
    "    return configs\n",
    "\n",
    "    # return [triton.Config(\n",
    "    #                             {\n",
    "    #                                 \"GROUP_SIZE_M\" : 8,\n",
    "    #                                 \"BLOCK_SIZE_M\" : 16,\n",
    "    #                                 \"BLOCK_SIZE_N\" : 16,\n",
    "    #                                 \"BLOCK_SIZE_K\" : 16,\n",
    "    #                             },\n",
    "    #                             num_stages=2, \n",
    "    #                             num_warps=4\n",
    "    #                         )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2683f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"GROUP_SIZE_M\" : 8,\n",
    "                \"BLOCK_SIZE_M\" : 64,\n",
    "                \"BLOCK_SIZE_N\" : 128,\n",
    "                \"BLOCK_SIZE_K\" : 16,\n",
    "            },\n",
    "            num_stages=2, \n",
    "            num_warps=4\n",
    "        )\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp16(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp16[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94af5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    # configs=_get_cuda_autotune_config(),\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"GROUP_SIZE_M\" : 8,\n",
    "                \"BLOCK_SIZE_M\" : 128,\n",
    "                \"BLOCK_SIZE_N\" : 128,\n",
    "                \"BLOCK_SIZE_K\" : 16,\n",
    "            },\n",
    "            num_stages=4, \n",
    "            num_warps=4\n",
    "        )\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_int4_fp16_scaled(\n",
    "        scales_ptr,\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    scales_ptrs = scales_ptr + pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        \n",
    "        scales = tl.load(scales_ptrs)\n",
    "        b = b.to(tl.float16) * scales[None, :]\n",
    "\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16_scaled(a, b, scales):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    \n",
    "    assert list(scales.shape) == [N,]\n",
    "    assert scales.dtype == torch.float16\n",
    "    \n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_int4_fp16_scaled[grid](\n",
    "        scales,\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0d095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    # configs=_get_cuda_autotune_config(),\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"GROUP_SIZE_M\" : 8,\n",
    "                \"BLOCK_SIZE_M\" : 64,\n",
    "                \"BLOCK_SIZE_N\" : 128,\n",
    "                \"BLOCK_SIZE_K\" : 16,\n",
    "            },\n",
    "            num_stages=2, \n",
    "            num_warps=4\n",
    "        )\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_int2_fp16(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 4) + tl.arange(0, BLOCK_SIZE_N // 4)) % (N // 4)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 4) * 2\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0x3\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x2\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int2_fp16(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int8\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 4\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_int2_fp16[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77e3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"GROUP_SIZE_M\" : 8,\n",
    "                \"BLOCK_SIZE_M\" : 64,\n",
    "                \"BLOCK_SIZE_N\" : 128,\n",
    "                \"BLOCK_SIZE_K\" : 16,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4\n",
    "        )\n",
    "    ],\n",
    "    # configs=_get_cuda_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp16_bigpack(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)) % (N // 8)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 8) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float16)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp16_bigpack(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int32\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 8\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp16_bigpack[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0b8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"GROUP_SIZE_M\" : 8,\n",
    "                \"BLOCK_SIZE_M\" : 64,\n",
    "                \"BLOCK_SIZE_N\" : 64,\n",
    "                \"BLOCK_SIZE_K\" : 16,\n",
    "            },\n",
    "            num_stages=2, \n",
    "            num_warps=4\n",
    "        )\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel_fp32(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bn = (pid_n * (BLOCK_SIZE_N // 2) + tl.arange(0, BLOCK_SIZE_N // 2)) % (N // 2)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # For bits unpack\n",
    "    shifter = tl.arange(0, 2) * 4\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)        \n",
    "        b_bits = tl.load(b_ptrs) #, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, BLOCK_SIZE_K, BLOCK_SIZE_N) - 0x8\n",
    "        b = b.to(tl.float32)\n",
    "        \n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    c = accumulator # .to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_int4_fp32(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float32\n",
    "    assert b.dtype == torch.int8\n",
    "\n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 2\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel_fp32[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa8dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int8_to_int4(bits):\n",
    "    v1 = (bits >> 4) & 0xF\n",
    "    v0 = bits & 0xF\n",
    "\n",
    "    w = torch.stack([v0, v1], dim=-1) - 0x8\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 2)\n",
    "\n",
    "\n",
    "def decode_int8_to_int2(bits):\n",
    "    v3 = (bits >> 6) & 0x3\n",
    "    v2 = (bits >> 4) & 0x3\n",
    "    v1 = (bits >> 2) & 0x3\n",
    "    v0 = bits & 0x3\n",
    "\n",
    "    w = torch.stack([v0, v1, v2, v3], dim=-1) - 0x2\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 4)\n",
    "\n",
    "\n",
    "def decode_int32_to_int4(bits):\n",
    "    v0 = bits & 0xF\n",
    "    v1 = (bits >> 4) & 0xF\n",
    "    v2 = (bits >> 8) & 0xF\n",
    "    v3 = (bits >> 12) & 0xF\n",
    "    v4 = (bits >> 16) & 0xF\n",
    "    v5 = (bits >> 20) & 0xF\n",
    "    v6 = (bits >> 24) & 0xF\n",
    "    v7 = (bits >> 28) & 0xF\n",
    "    \n",
    "    w = torch.stack([v0, v1, v2, v3, v4, v5, v6, v7], dim=-1) - 0x8\n",
    "    return w.reshape(bits.shape[0], bits.shape[1] * 8)\n",
    "\n",
    "\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# M, N, K = 3 * (4096,)\n",
    "# M, N, K = 3 * (1024,)\n",
    "# M, N, K = 3 * (16,)\n",
    "\n",
    "# y = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\")\n",
    "# x_decompressed = decode_int8_to_int4(x_compressed)\n",
    "\n",
    "# o1 = torch.matmul(y, x_decompressed.to(torch.float16)\n",
    "#                   )\n",
    "# o2 = triton_matmul_int4_fp16(y, x_compressed)\n",
    "# print(matmul_kernel_fp16.best_config)\n",
    "# # assert torch.all(torch.isclose(o1, o2))\n",
    "\n",
    "# o3 = triton_matmul_int4_fp32(y.float(), x_compressed)\n",
    "# print(matmul_kernel_fp32.best_config)\n",
    "# # assert torch.all(torch.isclose(o1, o3))\n",
    "\n",
    "# x_compressed_32bit = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\")\n",
    "# x_decompressed = decode_int32_to_int4(x_compressed_32bit)\n",
    "\n",
    "# o3 = triton_matmul_int4_fp16_bigpack(y, x_compressed_32bit)\n",
    "# print(matmul_kernel_fp16_bigpack.best_config)\n",
    "\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 4), dtype=torch.int8, device=\"cuda\")\n",
    "# x_decompressed = decode_int8_to_int2(x_compressed)\n",
    "\n",
    "# o1 = torch.matmul(y, x_decompressed.to(torch.float16))\n",
    "# o2 = triton_matmul_int2_fp16(y, x_compressed)\n",
    "# print(matmul_kernel_int2_fp16.best_config)\n",
    "\n",
    "\n",
    "# x_compressed = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\")\n",
    "# scales = torch.abs(torch.randn(N, dtype=torch.float16, device=\"cuda\"))\n",
    "# o2 = triton_matmul_int4_fp16_scaled(y, x_compressed, scales)\n",
    "# print(matmul_kernel_int4_fp16_scaled.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b8959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul-performance:\n",
      "         M       N       K  int2_fp16  int4_fp16_scaled\n",
      "0   4096.0   512.0   512.0   1.983855          1.826837\n",
      "1   8192.0  1024.0  1024.0   2.473057          2.391737\n",
      "2  16384.0  2048.0  2048.0   2.294672          2.250390\n",
      "3  32768.0  4096.0  4096.0   2.301487          2.297541\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BS = 8\n",
    "sizes = [512, 1024, 2048, 4096] #, 8192]\n",
    "configs = []\n",
    "configs.append(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[(BS * size, size, size) for size in sizes],\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "        line_vals=[\n",
    "            # \"torch_fp32\", \n",
    "            # \"torch_fp16\", \n",
    "            \"triton_int2_fp16\",\n",
    "            # \"triton_int4_fp16\",\n",
    "            \"triton_int4_fp16_scaled\",\n",
    "            # \"triton_int4_fp16_bigpack\",\n",
    "            # \"triton_int4_fp32\",\n",
    "            ],  # Label name for the lines\n",
    "        line_names=[\n",
    "            # \"torch_fp32\", \n",
    "            # \"torch_fp16\", \n",
    "            \"int2_fp16\",\n",
    "            # \"int4_fp16\", \n",
    "            \"int4_fp16_scaled\", \n",
    "            # \"int4_fp16_bigpack\",\n",
    "            # \"int4_fp32\", \n",
    "            ],  # Line styles\n",
    "        #styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        xlabel=\"Matrix size\",\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, K, N, provider):\n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\").contiguous() / (M * K)\n",
    "    # y_fp32 = torch.randn(M, K, dtype=torch.float32, device=\"cuda\").contiguous() / (M * K)\n",
    "\n",
    "    x_compressed_int2 = torch.randint(-128, 128, (K, N // 4), dtype=torch.int8, device=\"cuda\").contiguous()\n",
    "    x_compressed_int4 = torch.randint(-128, 128, (K, N // 2), dtype=torch.int8, device=\"cuda\").contiguous()\n",
    "    # x_compressed_int4_bigpack = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\").contiguous()\n",
    "    # x_decompressed_fp16 = decode_int8_to_int4(x_compressed_int4).reshape(K, N).to(torch.float16).contiguous()\n",
    "    # x_decompressed_fp32 = decode_int8_to_int4(x_compressed_int4).reshape(K, N).to(torch.float32).contiguous()\n",
    "    scales = torch.abs(torch.randn(N, dtype=torch.float16, device=\"cuda\"))\n",
    "\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch_fp32\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(y_fp32, x_decompressed_fp32), quantiles=quantiles)\n",
    "    if provider == \"torch_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(y_fp16, x_decompressed_fp16), quantiles=quantiles)\n",
    "    if provider == \"triton_int2_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int2_fp16(y_fp16, x_compressed_int2), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16(y_fp16, x_compressed_int4), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16_scaled\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16_scaled(y_fp16, x_compressed_int4, scales), quantiles=quantiles)\n",
    "    if provider == \"triton_int4_fp16_bigpack\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp16_bigpack(y_fp16, x_compressed_int4_bigpack), quantiles=quantiles)\n",
    "    # if provider == \"triton_int4_fp32\":\n",
    "    #     ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_int4_fp32(y_fp32, x_compressed), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(show_plots=False, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2f9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a455c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
