{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbe9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['TRITON_INTERPRET'] = '1'\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=get_cuda_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "        ACTIVATION: tl.constexpr  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Add some integer bound assumptions.\n",
    "    # This helps to guide integer analysis in the backend to optimize\n",
    "    # load/store offset address calculation\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16 #tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = leaky_relu(accumulator)\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `matmul_kernel`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n",
    "\n",
    "\n",
    "# %%\n",
    "# We can now create a convenience wrapper function that only takes two input tensors,\n",
    "# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n",
    "\n",
    "\n",
    "def matmul(a, b, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "        ACTIVATION=activation  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2683f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_trellis_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,  #\n",
    "        # Matrix dimensions\n",
    "        B, IN, OUT,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        #BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        #GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(axis=0)\n",
    "    grid_n = tl.cdiv(OUT, 16)\n",
    "    pid_m = pid // grid_n\n",
    "    pid_n = pid % grid_n\n",
    "\n",
    "\n",
    "    # pid = tl.program_id(axis=0)\n",
    "    # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    # num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    # num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    # group_id = pid // num_pid_in_group\n",
    "    # first_pid_m = group_id * GROUP_SIZE_M\n",
    "    # group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    # pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    # pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Add some integer bound assumptions.\n",
    "    # This helps to guide integer analysis in the backend to optimize\n",
    "    # load/store offset address calculation\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * 16 + tl.arange(0, 16)) % B\n",
    "    offs_bn = (pid_n * 64 + tl.arange(0, 64)) % (OUT * 4)\n",
    "    offs_ak = tl.arange(0, 16)\n",
    "    offs_bk = tl.arange(0, 1)\n",
    "\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)\n",
    "    b_ptrs_low = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    b_ptrs_high = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    \n",
    "    offs_high = 1 + tl.arange(0, BLOCK_SIZE // 4)\n",
    "    offs_high = bits_block_start + tl.where(offs_high >= BLOCK_SIZE // 4, offs_high - BLOCK_SIZE // 4, offs_high)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator_dtype = tl.float32 #tl.float16 #tl.float32\n",
    "    accumulator = tl.zeros((16, 16), dtype=accumulator_dtype)\n",
    "    for k in range(0, tl.cdiv(IN, 16)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs)#, mask=offs_k[None, :] < IN - k * 16, other=0.0)\n",
    "        bits_low = tl.load(b_ptrs_low)#, mask=offs_k[:, None] < IN - k * 16, other=0.0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        raise\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def matmul_trellis(a, b_compressed):\n",
    "    assert a.shape[1] == b_compressed.shape[0] * 16, \"Incompatible dimensions\"\n",
    "    assert b_compressed.is_contiguous(), \"Matrix B_compressed must be contiguous\"\n",
    "\n",
    "    B, IN = a.shape\n",
    "    OUT = b_compressed.shape[-1] // 4\n",
    "\n",
    "    # Init out ptr\n",
    "    c = torch.empty((B, OUT), device=b_compressed.device, dtype=torch.float16)\n",
    "\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(B, 16) * triton.cdiv(OUT, 16), )\n",
    "    matmul_trellis_kernel[grid](\n",
    "        a, b_compressed, c,  #\n",
    "        B, IN, OUT,  #\n",
    "        a.stride(0), \n",
    "        a.stride(1),  #\n",
    "        b_compressed.stride(0), \n",
    "        b_compressed.stride(1),  #\n",
    "        c.stride(0), \n",
    "        c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aa8dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "torch.Size([16, 16])\n",
      "tensor([[ 76, 192, 181,  97, 158,  99, 173,   8, 197, 183, 215, 112, 238, 164,\n",
      "          83,  18, 244, 131, 137, 166, 192,  87, 161,  85, 164,  22, 201, 147,\n",
      "           7,  64,  35,  70,  29,  52, 215,  66, 192, 152, 174, 234,  61, 113,\n",
      "         146,  76, 168, 166, 185, 168, 133,  98, 225, 213,  97,  35, 107, 237,\n",
      "          12, 154,   4,  75,  14,  52, 195, 179]], device='cuda:0',\n",
      "       dtype=torch.uint8)\n",
      "a_ptrs.shape: ['constexpr[16]', 'constexpr[16]']\n",
      "b_ptrs.shape: ['constexpr[1]', 'constexpr[64]'] [[945159424 945159425 945159426 945159427 945159428 945159429 945159430\n",
      "  945159431 945159432 945159433 945159434 945159435 945159436 945159437\n",
      "  945159438 945159439 945159440 945159441 945159442 945159443 945159444\n",
      "  945159445 945159446 945159447 945159448 945159449 945159450 945159451\n",
      "  945159452 945159453 945159454 945159455 945159456 945159457 945159458\n",
      "  945159459 945159460 945159461 945159462 945159463 945159464 945159465\n",
      "  945159466 945159467 945159468 945159469 945159470 945159471 945159472\n",
      "  945159473 945159474 945159475 945159476 945159477 945159478 945159479\n",
      "  945159480 945159481 945159482 945159483 945159484 945159485 945159486\n",
      "  945159487]]\n",
      "[[ 76 192 181  97 158  99 173   8 197 183 215 112 238 164  83  18 244 131\n",
      "  137 166 192  87 161  85 164  22 201 147   7  64  35  70  29  52 215  66\n",
      "  192 152 174 234  61 113 146  76 168 166 185 168 133  98 225 213  97  35\n",
      "  107 237  12 154   4  75  14  52 195 179]]\n"
     ]
    },
    {
     "ename": "InterpreterError",
     "evalue": "RuntimeError('No active exception to reraise')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1275\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1274\u001b[39m                 interpreter_builder.set_grid_idx(x, y, z)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mmatmul_trellis_kernel\u001b[39m\u001b[34m(a_ptr, b_ptr, c_ptr, B, IN, OUT, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# We accumulate along the K dimension.\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No active exception to reraise",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInterpreterError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(x_compressed)\n\u001b[32m     60\u001b[39m o1 = y @ x_decompressed.to(torch.float16)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m o2 = \u001b[43mmatmul_trellis\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_compressed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mmatmul_trellis\u001b[39m\u001b[34m(a, b_compressed)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# 1D launch kernel where each block gets its own program.\u001b[39;00m\n\u001b[32m    110\u001b[39m grid = \u001b[38;5;28;01mlambda\u001b[39;00m META: (triton.cdiv(B, \u001b[32m16\u001b[39m) * triton.cdiv(OUT, \u001b[32m16\u001b[39m), )\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43mmatmul_trellis_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_compressed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_compressed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_compressed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1279\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m triton.knobs.compilation.front_end_debugging:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InterpreterError(\u001b[38;5;28mrepr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1280\u001b[39m \u001b[38;5;66;03m# copy arguments back to propagate side-effects\u001b[39;00m\n\u001b[32m   1281\u001b[39m \u001b[38;5;28mself\u001b[39m._restore_args_dev(args_dev, args_hst, kwargs, kwargs_hst)\n",
      "\u001b[31mInterpreterError\u001b[39m: RuntimeError('No active exception to reraise')"
     ]
    }
   ],
   "source": [
    "def decode_uint16_to_int4_python(bits):\n",
    "    bits = bits.to(torch.int32)\n",
    "    val0 = (bits >> 12) & 0xF\n",
    "    val1 = (bits >> 8) & 0xF\n",
    "    val2 = (bits >> 4) & 0xF\n",
    "    val3 = bits & 0xF\n",
    "\n",
    "    sign0 = (val0 & 0x8) != 0\n",
    "    sign1 = (val1 & 0x8) != 0\n",
    "    sign2 = (val2 & 0x8) != 0\n",
    "    sign3 = (val3 & 0x8) != 0\n",
    "    \n",
    "    dtype = torch.int8\n",
    "\n",
    "    mag0 = (val0 & 0x7).to(dtype)\n",
    "    mag1 = (val1 & 0x7).to(dtype)\n",
    "    mag2 = (val2 & 0x7).to(dtype)\n",
    "    mag3 = (val3 & 0x7).to(dtype)\n",
    "    \n",
    "    w0 = torch.where(sign0, -mag0, mag0)\n",
    "    w1 = torch.where(sign1, -mag1, mag1)\n",
    "    w2 = torch.where(sign2, -mag2, mag2)\n",
    "    w3 = torch.where(sign3, -mag3, mag3)\n",
    "\n",
    "    return torch.stack([w0, w1, w2, w3], dim=-1).reshape(-1)\n",
    "\n",
    "\n",
    "torch.compile\n",
    "def permute_16bits(x):\n",
    "    x = x.to(torch.int32) * 34038481\n",
    "    x = (x >> 9) # & 0xFFFF\n",
    "    return x.to(torch.uint16)\n",
    "\n",
    "\n",
    "@torch.compile()\n",
    "def decode_trellis_python(bits, bits_block_size=64):\n",
    "    bits = bits.reshape(-1, bits_block_size)\n",
    "    even_codes = bits.view(torch.uint16)\n",
    "    odd_codes = bits.roll(shifts=-1, dims=-1).view(torch.uint16)\n",
    "\n",
    "    codes = torch.stack([even_codes, odd_codes], dim=-1).reshape(-1)\n",
    "\n",
    "    bits = permute_16bits(codes) # << bottleneck operation\n",
    "    return decode_uint16_to_int4_python(bits)\n",
    "\n",
    "\n",
    "w_matrix_size = ()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_compressed = torch.randint(0, 255, (1, 64), dtype=torch.uint8, device=\"cuda\")\n",
    "x_decompressed = decode_trellis_python(x_compressed.view(-1)).reshape(16, 16)\n",
    "y = torch.randn(16, 16, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "print(x_compressed.shape)\n",
    "print(x_decompressed.shape)\n",
    "\n",
    "#print(y)\n",
    "print(x_compressed)\n",
    "\n",
    "o1 = y @ x_decompressed.to(torch.float16)\n",
    "o2 = matmul_trellis(y, x_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e916b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52236e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b6d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8959f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m a = torch.rand((\u001b[32m512\u001b[39m, \u001b[32m512\u001b[39m), device=DEVICE, dtype=torch.float16) - \u001b[32m0.5\u001b[39m\n\u001b[32m      3\u001b[39m b = torch.rand((\u001b[32m512\u001b[39m, \u001b[32m512\u001b[39m), device=DEVICE, dtype=torch.float16) - \u001b[32m0.5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m triton_output = \u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m torch_output = torch.matmul(a, b)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#print(f\"triton_output_with_fp16_inputs={triton_output}\")\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#print(f\"torch_output_with_fp16_inputs={torch_output}\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, activation)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# 1D launch kernel where each block gets its own program.\u001b[39;00m\n\u001b[32m    154\u001b[39m grid = \u001b[38;5;28;01mlambda\u001b[39;00m META: (triton.cdiv(M, META[\u001b[33m'\u001b[39m\u001b[33mBLOCK_SIZE_M\u001b[39m\u001b[33m'\u001b[39m]) * triton.cdiv(N, META[\u001b[33m'\u001b[39m\u001b[33mBLOCK_SIZE_N\u001b[39m\u001b[33m'\u001b[39m]), )\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[43mmatmul_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mACTIVATION\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/jit.py:390\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    385\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/autotuner.py:239\u001b[39m, in \u001b[36mAutotuner.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m             used_cached_result = \u001b[38;5;28mself\u001b[39m.check_disk_cache(key, pruned_configs, benchmark)\n\u001b[32m    238\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m             \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     config = \u001b[38;5;28mself\u001b[39m.cache[key]\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/autotuner.py:228\u001b[39m, in \u001b[36mAutotuner.run.<locals>.benchmark\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbenchmark\u001b[39m():\n\u001b[32m    227\u001b[39m     bench_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     timings = {config: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[32m    229\u001b[39m     bench_end = time.time()\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m.bench_time = bench_end - bench_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/autotuner.py:160\u001b[39m, in \u001b[36mAutotuner._bench\u001b[39m\u001b[34m(self, config, *args, **meta)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m.post_hook(full_nargs, exception=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (OutOfResources, CompileTimeAssertionFailure, PTXASError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/testing.py:185\u001b[39m, in \u001b[36mdo_bench\u001b[39m\u001b[34m(fn, warmup, rep, grad_to_none, quantiles, return_mode)\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# record time of `fn`\u001b[39;00m\n\u001b[32m    184\u001b[39m     start_event[i].record()\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     end_event[i].record()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Record clocks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/autotuner.py:146\u001b[39m, in \u001b[36mAutotuner._bench.<locals>.kernel_call\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m.pre_hook(full_nargs)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1380\u001b[39m, in \u001b[36mInterpretedFunction.__init__.<locals>.run\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m grid = kwargs[\u001b[33m\"\u001b[39m\u001b[33mgrid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1379\u001b[39m fn = \u001b[38;5;28mself\u001b[39m.rewrite()\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGridExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1275\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1273\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grid[\u001b[32m2\u001b[39m]):\n\u001b[32m   1274\u001b[39m                 interpreter_builder.set_grid_idx(x, y, z)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m triton.knobs.compilation.front_end_debugging:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mmatmul_kernel\u001b[39m\u001b[34m(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, ACTIVATION)\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# Advance the ptrs to the next K block.\u001b[39;00m\n\u001b[32m    117\u001b[39m     a_ptrs += BLOCK_SIZE_K * stride_ak\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     b_ptrs += \u001b[43mBLOCK_SIZE_K\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_bk\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# You can fuse arbitrary activation functions here\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# while the accumulator is still in FP32!\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ACTIVATION == \u001b[33m\"\u001b[39m\u001b[33mleaky_relu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:781\u001b[39m, in \u001b[36m_patch_attr.<locals>.<lambda>\u001b[39m\u001b[34m(member, *args, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_patch_attr\u001b[39m(obj, name, member, builder):\n\u001b[32m    780\u001b[39m     semantic = TritonSemantic(builder)\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     new_member = \u001b[38;5;28;01mlambda\u001b[39;00m *args, member=member, **kwargs: (\u001b[43mmember\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m                                                                \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                                                                 \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                                                                 \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_semantic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_semantic\u001b[49m\u001b[43m=\u001b[49m\u001b[43msemantic\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, name, new_member)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:42\u001b[39m, in \u001b[36mbuiltin.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_semantic\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mor\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33m_semantic\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDid you forget to add @triton.jit ? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m(`_semantic` argument must be provided outside of JIT functions.)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:920\u001b[39m, in \u001b[36mtensor.__rmul__\u001b[39m\u001b[34m(self, other, _semantic)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;129m@builtin\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__rmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, _semantic=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_overflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_semantic\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_semantic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:781\u001b[39m, in \u001b[36m_patch_attr.<locals>.<lambda>\u001b[39m\u001b[34m(member, *args, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_patch_attr\u001b[39m(obj, name, member, builder):\n\u001b[32m    780\u001b[39m     semantic = TritonSemantic(builder)\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     new_member = \u001b[38;5;28;01mlambda\u001b[39;00m *args, member=member, **kwargs: (\u001b[43mmember\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m                                                                \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                                                                 \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                                                                 \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_semantic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_semantic\u001b[49m\u001b[43m=\u001b[49m\u001b[43msemantic\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, name, new_member)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:42\u001b[39m, in \u001b[36mbuiltin.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_semantic\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mor\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33m_semantic\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDid you forget to add @triton.jit ? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m(`_semantic` argument must be provided outside of JIT functions.)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:2482\u001b[39m, in \u001b[36mmul\u001b[39m\u001b[34m(x, y, sanitize_overflow, _semantic)\u001b[39m\n\u001b[32m   2480\u001b[39m x = _unwrap_if_constexpr(x)\n\u001b[32m   2481\u001b[39m y = _unwrap_if_constexpr(y)\n\u001b[32m-> \u001b[39m\u001b[32m2482\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_semantic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/semantic.py:282\u001b[39m, in \u001b[36mTritonSemantic.mul\u001b[39m\u001b[34m(self, input, other, sanitize_overflow)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m scalar_ty.is_int():\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sanitize_overflow:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbinary_op_sanitize_overflow_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmul\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensor(\u001b[38;5;28mself\u001b[39m.builder.create_mul(\u001b[38;5;28minput\u001b[39m.handle, other.handle), \u001b[38;5;28minput\u001b[39m.type)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscalar_ty\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/semantic.py:213\u001b[39m, in \u001b[36mTritonSemantic.binary_op_sanitize_overflow_impl\u001b[39m\u001b[34m(self, lhs, rhs, binary_op)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m lhs_sca_ty == rhs_sca_ty\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m lhs_sca_ty.is_int()\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m lhs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m rhs = \u001b[38;5;28mself\u001b[39m.cast(rhs, tl.int64)\n\u001b[32m    215\u001b[39m ret = binary_op(lhs, rhs, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/semantic.py:859\u001b[39m, in \u001b[36mTritonSemantic.cast\u001b[39m\u001b[34m(self, input, dst_ty, fp_downcast_rounding)\u001b[39m\n\u001b[32m    857\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.not_equal(\u001b[38;5;28minput\u001b[39m, _0)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_int_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_ty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign_extend\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    860\u001b[39m                            dst_ty)\n\u001b[32m    862\u001b[39m \u001b[38;5;66;03m# Casting standard floating types to integer types\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m src_sca_ty.is_standard_floating() \u001b[38;5;129;01mand\u001b[39;00m dst_sca_ty.is_int():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:448\u001b[39m, in \u001b[36mInterpreterBuilder.<lambda>\u001b[39m\u001b[34m(self, src, dst_type, is_signed)\u001b[39m\n\u001b[32m    446\u001b[39m create_fp_ext = \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, src, dst_type: \u001b[38;5;28mself\u001b[39m.cast_impl(src, dst_type)\n\u001b[32m    447\u001b[39m create_fp_trunc = \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, src, dst_type: \u001b[38;5;28mself\u001b[39m.cast_impl(src, dst_type)\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m create_int_cast = \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, src, dst_type, is_signed: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcast_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_fp_to_fp\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, dst_type, rounding_mode):\n\u001b[32m    451\u001b[39m     src_element_type = src.dtype.scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:440\u001b[39m, in \u001b[36mInterpreterBuilder.cast_impl\u001b[39m\u001b[34m(self, src, dst_type)\u001b[39m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorHandle(data, dst_type.scalar)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorHandle(src.data.astype(\u001b[43m_get_np_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_type\u001b[49m\u001b[43m)\u001b[49m), dst_type.scalar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:146\u001b[39m, in \u001b[36m_get_np_dtype\u001b[39m\u001b[34m(tt_dtype)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tt_dtype, tl.pointer_type):\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.dtype(np.uint64)\n\u001b[32m    144\u001b[39m np_types = {\n\u001b[32m    145\u001b[39m     tl.int1: np.dtype(\u001b[38;5;28mbool\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     tl.float16: \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    147\u001b[39m     tl.float32: np.dtype(np.float32),\n\u001b[32m    148\u001b[39m     tl.float64: np.dtype(np.float64),\n\u001b[32m    149\u001b[39m     tl.int8: np.dtype(np.int8),\n\u001b[32m    150\u001b[39m     tl.uint8: np.dtype(np.uint8),\n\u001b[32m    151\u001b[39m     tl.int16: np.dtype(np.int16),\n\u001b[32m    152\u001b[39m     tl.uint16: np.dtype(np.uint16),\n\u001b[32m    153\u001b[39m     tl.int32: np.dtype(np.int32),\n\u001b[32m    154\u001b[39m     tl.uint32: np.dtype(np.uint32),\n\u001b[32m    155\u001b[39m     tl.int64: np.dtype(np.int64),\n\u001b[32m    156\u001b[39m     tl.uint64: np.dtype(np.uint64),\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# bfloat16 types are stored as uint16\u001b[39;00m\n\u001b[32m    158\u001b[39m     tl.bfloat16: np.dtype(np.uint16),\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# float8 types are stored as uint8\u001b[39;00m\n\u001b[32m    160\u001b[39m     tl.float8e5: np.dtype(np.uint8),\n\u001b[32m    161\u001b[39m     tl.float8e5b16: np.dtype(np.uint8),\n\u001b[32m    162\u001b[39m     tl.float8e4nv: np.dtype(np.uint8),\n\u001b[32m    163\u001b[39m     tl.float8e4b8: np.dtype(np.uint8),\n\u001b[32m    164\u001b[39m     tl.float8e4b15: np.dtype(np.uint8),\n\u001b[32m    165\u001b[39m }\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tt_dtype, tl.block_type):\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tt_dtype.element_ty, tl.pointer_type):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.rand((512, 512), device=DEVICE, dtype=torch.float16) - 0.5\n",
    "b = torch.rand((512, 512), device=DEVICE, dtype=torch.float16) - 0.5\n",
    "triton_output = matmul(a, b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "#print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "#print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "\n",
    "if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n",
    "    print(\" Triton and Torch match\")\n",
    "else:\n",
    "    print(\" Triton and Torch differ\")\n",
    "\n",
    "TORCH_HAS_FP8 = hasattr(torch, \"float8_e5m2\")\n",
    "if TORCH_HAS_FP8:\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    a = a.to(torch.float8_e5m2)\n",
    "    # pre-transpose b for efficiency.\n",
    "    b = b.T\n",
    "    b = b.to(torch.float8_e5m2)\n",
    "    triton_output = matmul(a, b)\n",
    "    torch_output = torch.matmul(a.to(torch.float16), b.to(torch.float16))\n",
    "    #print(f\"triton_output_with_fp8_inputs={triton_output}\")\n",
    "    #print(f\"torch_output_with_fp8_inputs={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "        print(\" Triton and Torch match\")\n",
    "    else:\n",
    "        print(\" Triton and Torch differ\")\n",
    "\n",
    "# %%\n",
    "# Benchmark\n",
    "# ---------\n",
    "#\n",
    "# Square Matrix Performance\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# We can now compare the performance of our kernel against that of cuBLAS or rocBLAS. Here we focus on square matrices,\n",
    "# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n",
    "\n",
    "ref_lib = 'cuBLAS'\n",
    "\n",
    "configs = []\n",
    "for fp8_inputs in [False, True]:\n",
    "    if fp8_inputs and (not TORCH_HAS_FP8):\n",
    "        continue\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[256, 512, 1024, 4096, 8192],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[\"triton\"] if fp8_inputs else [ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "            line_names=[\"Triton\"] if fp8_inputs else [ref_lib, \"Triton\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"matmul-performance-\" +\n",
    "            (\"fp16\" if not fp8_inputs else \"fp8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={\"fp8_inputs\": fp8_inputs},\n",
    "        ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        b = b.T\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=False, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
