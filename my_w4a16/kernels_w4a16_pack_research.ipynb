{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2907d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 5070 Ti\n",
      "Compute Capability: 12.0\n",
      "Total Memory: 15.45 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "# os.environ['TRITON_INTERPRET'] = '1'\n",
    "import triton\n",
    "import triton.language as tl\n",
    "# import math\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    props = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    print(f\"Device: {props.name}\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    # print(f\"CUDA Cores: {props.multi_processor_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe188e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speedup_columns(df):\n",
    "    df_result = df.copy()\n",
    "    base_column = 'torch_fp16'\n",
    "    \n",
    "    new_column_order = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        new_column_order.append(column)\n",
    "        if column not in ['K', 'M', 'N', base_column]:\n",
    "            speedup_column = f\"{column}_speedup\"\n",
    "            df_result[speedup_column] = df[column] / df[base_column]\n",
    "            new_column_order.append(speedup_column)\n",
    "    df_result = df_result[new_column_order]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f56fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config_small_bs():\n",
    "    configs = []\n",
    "    for num_warps, num_stages in [\n",
    "        (4, 2),\n",
    "        (4, 3),\n",
    "        (4, 4),\n",
    "    ]:\n",
    "        for BLOCK_SIZE_M in [16]:\n",
    "            for BLOCK_SIZE_N in [16, 32, 64, 128]:\n",
    "                for BLOCK_SIZE_K in [128, 256, 512]:\n",
    "                    configs.append(\n",
    "                        triton.Config(\n",
    "                            {\n",
    "                                \"BLOCK_SIZE_M\" : BLOCK_SIZE_M,\n",
    "                                \"BLOCK_SIZE_N\" : BLOCK_SIZE_N,\n",
    "                                \"BLOCK_SIZE_K\" : BLOCK_SIZE_K,\n",
    "                            },\n",
    "                            num_stages=num_stages, \n",
    "                            num_warps=num_warps\n",
    "                        ),\n",
    "                    )\n",
    "    return configs\n",
    "\n",
    "\n",
    "@triton.jit()\n",
    "def get_pid_point_grouped(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "        GROUP_SIZE: tl.constexpr\n",
    "    ):\n",
    "    \n",
    "    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "\n",
    "    width = GROUP_SIZE * grid_n\n",
    "    group_id = pid // width\n",
    "    group_size = tl.minimum(grid_m - group_id * group_m, group_m)\n",
    "\n",
    "    pid_m = group_id * group_m + (pid % group_size)\n",
    "    pid_n = (pid % width) // group_size\n",
    "\n",
    "    return pid_m, pid_n\n",
    "\n",
    "\n",
    "@triton.jit()\n",
    "def get_pid_point_base(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "    ):\n",
    "    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    pid_m = 0\n",
    "    pid_n = pid % grid_n\n",
    "    return pid_m, pid_n\n",
    "\n",
    "\n",
    "@triton.jit()\n",
    "def get_pid_point_swizzle(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "    ):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_m = 0\n",
    "    block_id = pid // 64\n",
    "    in_block = pid % 64\n",
    "    s_block = in_block % 16\n",
    "    s_pos = in_block // 16\n",
    "    pid_n = 64 * block_id + (64 // 16) * s_block + s_pos\n",
    "    \n",
    "    return pid_m, pid_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c6bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_cuda_autotune_config_small_bs(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def kernel_w4t_a16_matmul_small(\n",
    "        a_ptr, b_ptr, c_ptr, scale_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_m, pid_n = get_pid_point_base(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N\n",
    "    )\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    scale = tl.load(scale_ptr)\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bk = tl.arange(0, BLOCK_SIZE_K // 8)\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    b_ptrs = b_ptr + ((offs_bk[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    shifter = tl.arange(0, 8) * 4\n",
    "\n",
    "    accumulator_dtype = tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "\n",
    "    a_mask0 = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n",
    "    b_mask0 = (offs_bn[None, :] < N) & ((offs_bk[:, None]) < K // 8)\n",
    "    a = tl.load(a_ptrs, mask=a_mask0, other=0.0, eviction_policy=\"evict_last\")\n",
    "    b_bits = tl.load(b_ptrs, mask=b_mask0, other=0)\n",
    "    for k_idx in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        next_k_offset = (k_idx + 1) * BLOCK_SIZE_K\n",
    "        \n",
    "        if k_idx + 1 < tl.cdiv(K, BLOCK_SIZE_K):\n",
    "            a_next_mask = (offs_am[:, None] < M) & (offs_k[None, :] + next_k_offset < K)\n",
    "            b_next_mask = (offs_bn[None, :] < N) & ((offs_bk[:, None] + (next_k_offset // 8)) < K // 8)\n",
    "            a_next = tl.load(a_ptrs + next_k_offset * stride_ak, mask=a_next_mask, other=0.0, eviction_policy=\"evict_last\")\n",
    "            b_bits_next = tl.load(b_ptrs + (next_k_offset // 8) * stride_bk, mask=b_next_mask, other=0)\n",
    "        else:\n",
    "            a_next = tl.zeros_like(a)\n",
    "            b_bits_next = tl.zeros_like(b_bits)\n",
    "\n",
    "        b = (b_bits[:, None, :] >> shifter[None, :, None]) & 0xF\n",
    "        b = tl.reshape(b, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n",
    "        b = (b.to(tl.float16) - 7.5)\n",
    "        \n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "\n",
    "        a = a_next\n",
    "        b_bits = b_bits_next\n",
    "\n",
    "    accumulator = accumulator * scale\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, accumulator.to(tl.float16), mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_w4a16_tensor(a, b, scale):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0] * 8, \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int32\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1]\n",
    "    \n",
    "    assert scale.dtype == torch.float16\n",
    "    assert len(scale.shape) == 1\n",
    "    assert scale.shape[0] == 1\n",
    "    \n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    kernel_w4t_a16_matmul_small[grid](\n",
    "        a, b, c, scale, #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e449631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_cuda_autotune_config_small_bs(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def kernel_w4t_a16_matmul_small_test2(\n",
    "        a_ptr, b_ptr, c_ptr, scale_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_m, pid_n = get_pid_point_base(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N\n",
    "    )\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    scale = tl.load(scale_ptr)\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bk = tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N // 8 + tl.arange(0, BLOCK_SIZE_N // 8)) % (N // 8)\n",
    "    b_ptrs = b_ptr + ((offs_bk[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    shifter = tl.arange(0, 8) * 4\n",
    "\n",
    "    accumulator_dtype = tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "\n",
    "    a_mask0 = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n",
    "    b_mask0 = (offs_bn[None, :] < (N // 8)) & ((offs_bk[:, None]) < K)\n",
    "    a = tl.load(a_ptrs, mask=a_mask0, other=0.0, eviction_policy=\"evict_last\")\n",
    "    b_bits = tl.load(b_ptrs, mask=b_mask0, other=0)\n",
    "    for k_idx in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        next_k_offset = (k_idx + 1) * BLOCK_SIZE_K\n",
    "        \n",
    "        if k_idx + 1 < tl.cdiv(K, BLOCK_SIZE_K):\n",
    "            a_next_mask = (offs_am[:, None] < M) & (offs_k[None, :] + next_k_offset < K)\n",
    "            b_next_mask = (offs_bn[None, :] < N // 8) & ((offs_bk[:, None] + (next_k_offset)) < K)\n",
    "            a_next = tl.load(a_ptrs + next_k_offset * stride_ak, mask=a_next_mask, other=0.0, eviction_policy=\"evict_last\")\n",
    "            b_bits_next = tl.load(b_ptrs + next_k_offset * stride_bk, mask=b_next_mask, other=0)\n",
    "        else:\n",
    "            a_next = tl.zeros_like(a)\n",
    "            b_bits_next = tl.zeros_like(b_bits)\n",
    "\n",
    "        b = (b_bits[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "        b = tl.reshape(b, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n",
    "        b = (b.to(tl.float16) - 7.5)\n",
    "        \n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "\n",
    "        a = a_next\n",
    "        b_bits = b_bits_next\n",
    "\n",
    "    accumulator = accumulator * scale\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, accumulator.to(tl.float16), mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_w4a16_tensor_test2(a, b, scale):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int32\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[1] * 8\n",
    "    \n",
    "    assert scale.dtype == torch.float16\n",
    "    assert len(scale.shape) == 1\n",
    "    assert scale.shape[0] == 1\n",
    "    \n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    kernel_w4t_a16_matmul_small_test2[grid](\n",
    "        a, b, c, scale, #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27d7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_cuda_autotune_config_small_bs(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def kernel_w4t_a16_matmul_small_test3(\n",
    "        a_ptr, b_ptr, c_ptr, scale_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bn, stride_bk,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_m, pid_n = get_pid_point_base(\n",
    "        pid,\n",
    "        M, N,\n",
    "        BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N\n",
    "    )\n",
    "\n",
    "    tl.assume(pid_m >= 0)\n",
    "    tl.assume(pid_n >= 0)\n",
    "    tl.assume(stride_am > 0)\n",
    "    tl.assume(stride_ak > 0)\n",
    "    tl.assume(stride_bn > 0)\n",
    "    tl.assume(stride_bk > 0)\n",
    "    tl.assume(stride_cm > 0)\n",
    "    tl.assume(stride_cn > 0)\n",
    "\n",
    "    scale = tl.load(scale_ptr)\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    \n",
    "    offs_bk = tl.arange(0, BLOCK_SIZE_K // 8)\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    # b_ptrs = b_ptr + ((offs_bk[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    b_ptrs = b_ptr + (offs_bn[:, None] * stride_bn + (offs_bk[None, :]) * stride_bk)\n",
    "\n",
    "    shifter = tl.arange(0, 8) * 4\n",
    "\n",
    "    accumulator_dtype = tl.float32\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=accumulator_dtype)\n",
    "\n",
    "    a_mask0 = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n",
    "    b_mask0 = (offs_bn[:, None] < N) & ((offs_bk[None, :]) < K // 8)\n",
    "    a = tl.load(a_ptrs, mask=a_mask0, other=0.0, eviction_policy=\"evict_last\")\n",
    "    b_bits = tl.load(b_ptrs, mask=b_mask0, other=0)\n",
    "    for k_idx in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        next_k_offset = (k_idx + 1) * BLOCK_SIZE_K\n",
    "        \n",
    "        if k_idx + 1 < tl.cdiv(K, BLOCK_SIZE_K):\n",
    "            a_next_mask = (offs_am[:, None] < M) & (offs_k[None, :] + next_k_offset < K)\n",
    "            b_next_mask = (offs_bn[:, None] < N) & ((offs_bk[None, :] + (next_k_offset // 8)) < K // 8)\n",
    "            a_next = tl.load(a_ptrs + next_k_offset * stride_ak, mask=a_next_mask, other=0.0, eviction_policy=\"evict_last\")\n",
    "            b_bits_next = tl.load(b_ptrs + (next_k_offset // 8) * stride_bk, mask=b_next_mask, other=0)\n",
    "        else:\n",
    "            a_next = tl.zeros_like(a)\n",
    "            b_bits_next = tl.zeros_like(b_bits)\n",
    "\n",
    "        b = (b_bits[:, None, :] >> shifter[None, :, None]) & 0xF\n",
    "        b = tl.reshape(b, (BLOCK_SIZE_N, BLOCK_SIZE_K)).trans(1, 0)\n",
    "        b = (b.to(tl.float16) - 7.5)\n",
    "        \n",
    "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n",
    "\n",
    "        a = a_next\n",
    "        b_bits = b_bits_next\n",
    "\n",
    "    accumulator = accumulator * scale\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, accumulator.to(tl.float16), mask=c_mask)\n",
    "\n",
    "\n",
    "def triton_matmul_w4a16_tensor_test3(a, b, scale):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[1] * 8, \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "\n",
    "    assert a.dtype == torch.float16\n",
    "    assert b.dtype == torch.int32\n",
    "    \n",
    "    M, K = a.shape\n",
    "    N = b.shape[0]\n",
    "    \n",
    "    assert scale.dtype == torch.float16\n",
    "    assert len(scale.shape) == 1\n",
    "    assert scale.shape[0] == 1\n",
    "    \n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    kernel_w4t_a16_matmul_small_test3[grid](\n",
    "        a, b, c, scale, #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab0e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def deepseek_dist(x, y):\n",
    "    x, y = x.double(), y.double()\n",
    "    denom = (x * x + y * y).sum()\n",
    "    sim = 2 * (x * y).sum() / denom\n",
    "    return 1 - sim\n",
    "\n",
    "\n",
    "def run_w4a16_tensor_test(M, N, K, print_dist=False):\n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "    x_compressed = torch.randint(-2**31, 2**31, (K // 8, N), dtype=torch.int32, device=\"cuda\")\n",
    "    tensor_scale = torch.randn(1, dtype=torch.float16, device=\"cuda\").abs()\n",
    "\n",
    "    # triton\n",
    "    triton_out = triton_matmul_w4a16_tensor(y_fp16, x_compressed, tensor_scale)\n",
    "\n",
    "    # torch\n",
    "    shifter = (torch.arange(0, 8) * 4).cuda()\n",
    "    x_decompressed = (x_compressed[:, None, :] >> shifter[None, :, None]) & 0xF\n",
    "    x_decompressed = x_decompressed.reshape(K, N).to(torch.float16) - 7.5\n",
    "    torch_out = (y_fp16 @ x_decompressed) * tensor_scale\n",
    "\n",
    "    # results\n",
    "    dist = deepseek_dist(triton_out, torch_out)\n",
    "    \n",
    "    if print_dist:\n",
    "        print(f\"[M x K x N]: [{M} x {K} x {N}], dist = {dist}\")\n",
    "        \n",
    "    assert dist < 0.001\n",
    "\n",
    "\n",
    "def run_w4a16_tensor_test2_test(M, N, K, print_dist=False):\n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "    x_compressed = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\")\n",
    "    tensor_scale = torch.randn(1, dtype=torch.float16, device=\"cuda\").abs()\n",
    "\n",
    "    # triton\n",
    "    triton_out = triton_matmul_w4a16_tensor_test2(y_fp16, x_compressed, tensor_scale)\n",
    "\n",
    "    # torch\n",
    "    shifter = (torch.arange(0, 8) * 4).cuda()\n",
    "    x_decompressed = (x_compressed[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "    x_decompressed = x_decompressed.reshape(K, N).to(torch.float16) - 7.5\n",
    "    torch_out = (y_fp16 @ x_decompressed) * tensor_scale\n",
    "\n",
    "    # results\n",
    "    dist = deepseek_dist(triton_out, torch_out)\n",
    "    \n",
    "    if print_dist:\n",
    "        print(f\"[M x K x N]: [{M} x {K} x {N}], dist = {dist}\")\n",
    "        \n",
    "    assert dist < 0.001\n",
    "\n",
    "\n",
    "def run_w4a16_tensor_test3_test(M, N, K, print_dist=False):\n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "    x_compressed = torch.randint(-2**31, 2**31, (N, K // 8), dtype=torch.int32, device=\"cuda\")\n",
    "    tensor_scale = torch.randn(1, dtype=torch.float16, device=\"cuda\").abs()\n",
    "\n",
    "    # triton\n",
    "    triton_out = triton_matmul_w4a16_tensor_test3(y_fp16, x_compressed, tensor_scale)\n",
    "\n",
    "    # torch\n",
    "    shifter = (torch.arange(0, 8) * 4).cuda()\n",
    "    x_decompressed = (x_compressed[:, :, None] >> shifter[None, None, :]) & 0xF\n",
    "    x_decompressed = x_decompressed.reshape(K, N).to(torch.float16) - 7.5\n",
    "    torch_out = (y_fp16 @ x_decompressed) * tensor_scale\n",
    "\n",
    "    # results\n",
    "    dist = deepseek_dist(triton_out, torch_out)\n",
    "    \n",
    "    if print_dist:\n",
    "        print(f\"[M x K x N]: [{M} x {K} x {N}], dist = {dist}\")\n",
    "        \n",
    "    assert dist < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715a8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 1\n",
    "sizes = [2**11, 2**12, 2**13, 2**14]\n",
    "# sizes = [2**11,] #, 2**12, 2**13, 2**14]\n",
    "\n",
    "BSs = [1, 16, 32, 128, 1024]\n",
    "size = 4096\n",
    "\n",
    "llama_sizes = [\n",
    "    (8, 11008, 4096),\n",
    "    (8, 4096, 11008),\n",
    "    (8, 4096, 4096),\n",
    "]\n",
    "\n",
    "experiments = [\n",
    "            \"torch_fp16\",\n",
    "            # \"triton_w4a16_tensor\",\n",
    "            # \"triton_w4a16_tensor_test2\",\n",
    "            \"triton_w4a16_tensor_test3\",\n",
    "            ]\n",
    "\n",
    "configs = []\n",
    "configs.append(\n",
    "    triton.testing.Benchmark(\n",
    "        # x_names=[\"M\", \"K\", \"N\"],\n",
    "        # x_vals=llama_sizes,\n",
    "\n",
    "        x_names=[\"K\", \"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[(size, BS, size) for size in sizes],\n",
    "\n",
    "        # x_names=[\"M\", \"K\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        # x_vals=[(BS, size, size) for BS in BSs],\n",
    "        # x_log=True,\n",
    "\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=experiments,\n",
    "        line_names=experiments,\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        xlabel=\"Matrix size\",\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, K, N, provider):\n",
    "    y_fp16 = torch.randn(M, K, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "    x_fp16 = torch.randn(K, N, dtype=torch.float16, device=\"cuda\") / (M * K)\n",
    "    \n",
    "    # x_compressed_4bit = torch.randint(-2**31, 2**31, (K // 8, N), dtype=torch.int32, device=\"cuda\")\n",
    "    x_compressed_4bit = torch.randint(-2**31, 2**31, (N, K // 8), dtype=torch.int32, device=\"cuda\").T\n",
    "    \n",
    "    # x_compressed_4bit_test2 = torch.randint(-2**31, 2**31, (K, N // 8), dtype=torch.int32, device=\"cuda\")\n",
    "    x_compressed_4bit_test2 = torch.randint(-2**31, 2**31, (N // 8, K), dtype=torch.int32, device=\"cuda\").T\n",
    "    \n",
    "    # x_compressed_4bit_test3 = torch.randint(-2**31, 2**31, (N, K // 8), dtype=torch.int32, device=\"cuda\")\n",
    "    x_compressed_4bit_test3 = torch.randint(-2**31, 2**31, (K // 8, N), dtype=torch.int32, device=\"cuda\").T\n",
    "    \n",
    "    \n",
    "    # x_compressed_4bit_test4 = torch.randint(-2**31, 2**31, (N // 8, K), dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "\n",
    "    \n",
    "    tensor_scale = torch.randn(1, dtype=torch.float16, device=\"cuda\").abs()\n",
    "    # channel_scale = torch.randn(N, dtype=torch.float16, device=\"cuda\")\n",
    "    # group256_scale = torch.randn((K // 256, N), dtype=torch.float16, device=\"cuda\")\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch_fp16\":\n",
    "        print(f\"\\n[M x K x N]: [{M} x {K} x {N}]\")\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(y_fp16, x_fp16), quantiles=quantiles)\n",
    "    \n",
    "    if provider == \"triton_w4a16_tensor\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_w4a16_tensor(y_fp16, x_compressed_4bit, tensor_scale), quantiles=quantiles)\n",
    "        print(\"matmul_kernel_w4a16_tensor:\", kernel_w4t_a16_matmul_small.best_config)\n",
    "\n",
    "    if provider == \"triton_w4a16_tensor_test2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_w4a16_tensor_test2(y_fp16, x_compressed_4bit_test2, tensor_scale), quantiles=quantiles)\n",
    "        print(\"matmul_kernel_w4a16_tensor_test2:\", kernel_w4t_a16_matmul_small_test2.best_config)\n",
    "\n",
    "    if provider == \"triton_w4a16_tensor_test3\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_w4a16_tensor_test3(y_fp16, x_compressed_4bit_test3, tensor_scale), quantiles=quantiles)\n",
    "        print(\"matmul_kernel_w4a16_tensor_test3:\", kernel_w4t_a16_matmul_small_test3.best_config)\n",
    "\n",
    "    if provider == \"triton_w4a16_channel\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_w4a16_channel(y_fp16, x_compressed_4bit, channel_scale), quantiles=quantiles)\n",
    "        print(\"matmul_kernel_w4a16_channel:\", matmul_kernel_w4a16_channel.best_config)\n",
    "\n",
    "    if provider == \"triton_w4a16_gs256\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_matmul_w4a16_gs256(y_fp16, x_compressed_4bit, group256_scale), quantiles=quantiles)\n",
    "        print(\"matmul_kernel_w4a16_gs256:\", matmul_kernel_w4a16_gs256.best_config)\n",
    "\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "# bench_data = benchmark.run(show_plots=False, print_data=True, return_df=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3036fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M x K x N]: [1 x 2048 x 2048], dist = 1.0446353062541676\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m M, N, K = BS, size, size\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# run_w4a16_tensor_test(M, N, K, print_dist=True)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# run_w4a16_tensor_test2_test(M, N, K, print_dist=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mrun_w4a16_tensor_test3_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_dist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrun_w4a16_tensor_test3_test\u001b[39m\u001b[34m(M, N, K, print_dist)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m print_dist:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[M x K x N]: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], dist = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdist\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dist < \u001b[32m0.001\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    M, N, K = BS, size, size\n",
    "    # run_w4a16_tensor_test(M, N, K, print_dist=True)\n",
    "    # run_w4a16_tensor_test2_test(M, N, K, print_dist=True)\n",
    "    run_w4a16_tensor_test3_test(M, N, K, print_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b47a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>torch_fp16</th>\n",
       "      <th>triton_w4a16_tensor_test3</th>\n",
       "      <th>triton_w4a16_tensor_test3_speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.229347</td>\n",
       "      <td>0.842907</td>\n",
       "      <td>3.675241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.399077</td>\n",
       "      <td>1.638400</td>\n",
       "      <td>4.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>0.606815</td>\n",
       "      <td>2.114065</td>\n",
       "      <td>3.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16384.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>0.802008</td>\n",
       "      <td>2.561016</td>\n",
       "      <td>3.193253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         K    M        N  torch_fp16  triton_w4a16_tensor_test3  \\\n",
       "0   2048.0  1.0   2048.0    0.229347                   0.842907   \n",
       "1   4096.0  1.0   4096.0    0.399077                   1.638400   \n",
       "2   8192.0  1.0   8192.0    0.606815                   2.114065   \n",
       "3  16384.0  1.0  16384.0    0.802008                   2.561016   \n",
       "\n",
       "   triton_w4a16_tensor_test3_speedup  \n",
       "0                           3.675241  \n",
       "1                           4.105469  \n",
       "2                           3.483871  \n",
       "3                           3.193253  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_speedup_columns(bench_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d532aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cuda_autotune_config():\n",
    "#     configs = []\n",
    "#     for num_warps, num_stages in [\n",
    "#         (4, 2),\n",
    "#         (4, 3),\n",
    "#         (4, 4),\n",
    "#         # (4, 5),\n",
    "#         # (8, 2),\n",
    "#         # (8, 4),\n",
    "#     ]:\n",
    "#         # configs.append(\n",
    "#         #     triton.Config({\"GROUP_SIZE_M\" : 8, \"BLOCK_SIZE_M\" : 64, \"BLOCK_SIZE_N\" : 128, \"BLOCK_SIZE_K\" : 64}, num_stages=num_stages, num_warps=num_warps),\n",
    "#         # )\n",
    "#         for GROUP_SIZE_M in [1]:\n",
    "#             for BLOCK_SIZE_M in [16]:\n",
    "#                 for BLOCK_SIZE_N in [16, 32, 64, 128]:\n",
    "#                     for BLOCK_SIZE_K in [128, 256, 512]:\n",
    "#                         configs.append(\n",
    "#                             triton.Config(\n",
    "#                                 {\n",
    "#                                     \"GROUP_SIZE_M\" : GROUP_SIZE_M,\n",
    "#                                     \"BLOCK_SIZE_M\" : BLOCK_SIZE_M,\n",
    "#                                     \"BLOCK_SIZE_N\" : BLOCK_SIZE_N,\n",
    "#                                     \"BLOCK_SIZE_K\" : BLOCK_SIZE_K,\n",
    "#                                 }, \n",
    "#                                 num_stages=num_stages, \n",
    "#                                 num_warps=num_warps\n",
    "#                             ),\n",
    "#                         )                        \n",
    "#     return configs\n",
    "#     return [triton.Config(\n",
    "#                                 {\n",
    "#                                     \"GROUP_SIZE_M\" : 1,\n",
    "#                                     \"BLOCK_SIZE_M\" : 16,\n",
    "#                                     \"BLOCK_SIZE_N\" : 32,\n",
    "#                                     \"BLOCK_SIZE_K\" : 128,\n",
    "#                                 },\n",
    "#                                 num_stages=4,\n",
    "#                                 num_warps=4\n",
    "#                             )]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
