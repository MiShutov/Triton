{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['TRITON_INTERPRET'] = '1'\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e4d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_tensor(tensor, tile_size):\n",
    "    \"\"\"\n",
    "    Разбивает тензор на тайлы и собирает их в новый тензор.\n",
    "    \n",
    "    Args:\n",
    "        tensor: torch.Tensor размером [M, N]\n",
    "        tile_size: размер тайла K\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor размером [M//K, N//K, K**2]\n",
    "    \"\"\"\n",
    "    M, N = tensor.shape\n",
    "    # Проверяем, что размеры делятся нацело\n",
    "    assert M % tile_size == 0 and N % tile_size == 0, \\\n",
    "        f\"Размеры {M}x{N} должны делиться на {tile_size} нацело\"\n",
    "    \n",
    "    # Разбиваем на тайлы\n",
    "    tiles = tensor.unfold(0, tile_size, tile_size).unfold(1, tile_size, tile_size)\n",
    "    # tiles.shape = [M//K, N//K, K, K]\n",
    "    \n",
    "    # Преобразуем в [M//K, N//K, K**2]\n",
    "    tiles_flat = tiles.contiguous().view(M // tile_size, N // tile_size, tile_size ** 2)\n",
    "    \n",
    "    return tiles_flat\n",
    "\n",
    "\n",
    "# Альтернативная версия обратной функции с использованием reshape и permute\n",
    "def untile_tensor_fast(tiled_tensor, original_shape, tile_size):\n",
    "    \"\"\"\n",
    "    Быстрая версия восстановления тензора из тайлов.\n",
    "    \n",
    "    Args:\n",
    "        tiled_tensor: torch.Tensor размером [M//K, N//K, K**2]\n",
    "        original_shape: кортеж (M, N)\n",
    "        tile_size: размер тайла K\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor размером [M, N]\n",
    "    \"\"\"\n",
    "    M, N = original_shape\n",
    "    num_tiles_h = M // tile_size\n",
    "    num_tiles_w = N // tile_size\n",
    "    \n",
    "    # Преобразуем в [M//K, N//K, K, K]\n",
    "    tiles = tiled_tensor.view(num_tiles_h, num_tiles_w, tile_size, tile_size)\n",
    "    \n",
    "    # Переупорядочиваем в [M, N]\n",
    "    reconstructed = tiles.permute(0, 2, 1, 3).contiguous().view(M, N)\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "\n",
    "# M, N, K = 6, 8, 2\n",
    "# original_tensor = torch.arange(M * N).float().view(M, N)\n",
    "# print(\"Исходный тензор:\")\n",
    "# print(original_tensor)\n",
    "# print(f\"Форма: {original_tensor.shape}\")\n",
    "\n",
    "# # Прямое преобразование\n",
    "# tiled = tile_tensor(original_tensor, K)\n",
    "# print(f\"\\nТайлы:\")\n",
    "# print(tiled)\n",
    "# print(f\"Форма тайлов: {tiled.shape}\")\n",
    "\n",
    "# # Обратное преобразование\n",
    "# reconstructed = untile_tensor_fast(tiled, (M, N), K)\n",
    "# print(f\"\\nВосстановленный тензор:\")\n",
    "# print(reconstructed)\n",
    "# print(f\"Форма: {reconstructed.shape}\")\n",
    "\n",
    "# # Проверка корректности\n",
    "# print(f\"\\nВосстановление корректно: {torch.allclose(original_tensor, reconstructed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8ad642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BITS_PER_VALUE: 2\n",
      "TILE_BITS: 512\n",
      "x.shape: torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_INT16_BITS = 16\n",
    "\n",
    "M = 16 # 512\n",
    "N = 16 # 512\n",
    "\n",
    "M_TILE_SIZE = 16\n",
    "N_TILE_SIZE = 16\n",
    "\n",
    "L = 16\n",
    "BIT_SHIFT = 8\n",
    "\n",
    "BITS_PER_VALUE = L // BIT_SHIFT\n",
    "TILE_BITS = M_TILE_SIZE * N_TILE_SIZE * BITS_PER_VALUE\n",
    "\n",
    "print(\"BITS_PER_VALUE:\", BITS_PER_VALUE)\n",
    "print(\"TILE_BITS:\", TILE_BITS)\n",
    "\n",
    "compessed = torch.randint(\n",
    "    0, \n",
    "    2**15 - 1, \n",
    "    (M // M_TILE_SIZE, N // N_TILE_SIZE, TILE_BITS // _INT16_BITS), \n",
    "    dtype=torch.uint16\n",
    ")\n",
    "\n",
    "B = 16\n",
    "x = torch.randn(B, N)\n",
    "print(\"x.shape:\", x.shape)\n",
    "\n",
    "compessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13951a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ec3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fp4_python(bits):\n",
    "    val0 = (bits >> 12) & 0xF\n",
    "    val1 = (bits >> 8) & 0xF\n",
    "    val2 = (bits >> 4) & 0xF\n",
    "    val3 = bits & 0xF\n",
    "    \n",
    "    sign0 = (val0 & 0x8) != 0\n",
    "    sign1 = (val1 & 0x8) != 0\n",
    "    sign2 = (val2 & 0x8) != 0\n",
    "    sign3 = (val3 & 0x8) != 0\n",
    "    \n",
    "    mag0 = (val0 & 0x7).to(torch.float16)\n",
    "    mag1 = (val1 & 0x7).to(torch.float16)\n",
    "    mag2 = (val2 & 0x7).to(torch.float16)\n",
    "    mag3 = (val3 & 0x7).to(torch.float16)\n",
    "    \n",
    "    w0 = torch.where(sign0, -mag0, mag0)\n",
    "    w1 = torch.where(sign1, -mag1, mag1)\n",
    "    w2 = torch.where(sign2, -mag2, mag2)\n",
    "    w3 = torch.where(sign3, -mag3, mag3)\n",
    "\n",
    "    return torch.stack([w0, w1, w2, w3], dim=-1)\n",
    "\n",
    "\n",
    "def matmul_python(bits, activations):\n",
    "    N = bits.shape[0]\n",
    "    fp4_matrix = decode_fp4_python(bits).reshape(N, 16, 16)\n",
    "    result = torch.matmul(activations, fp4_matrix)\n",
    "    return result\n",
    "\n",
    "@triton.jit\n",
    "def decode_fp4_triton(bits):\n",
    "    \"\"\"Декодирует 16-битное значение в 4 значения FP4\"\"\"\n",
    "    val0 = (bits >> 12) & 0xF\n",
    "    val1 = (bits >> 8) & 0xF\n",
    "    val2 = (bits >> 4) & 0xF\n",
    "    val3 = bits & 0xF\n",
    "    \n",
    "    sign0 = (val0 & 0x8) != 0\n",
    "    sign1 = (val1 & 0x8) != 0\n",
    "    sign2 = (val2 & 0x8) != 0\n",
    "    sign3 = (val3 & 0x8) != 0\n",
    "    \n",
    "    mag0 = (val0 & 0x7)\n",
    "    mag1 = (val1 & 0x7)\n",
    "    mag2 = (val2 & 0x7)\n",
    "    mag3 = (val3 & 0x7)\n",
    "    \n",
    "    w0 = tl.where(sign0, -mag0.to(tl.float16), mag0.to(tl.float16))\n",
    "    w1 = tl.where(sign1, -mag1.to(tl.float16), mag1.to(tl.float16))\n",
    "    w2 = tl.where(sign2, -mag2.to(tl.float16), mag2.to(tl.float16))\n",
    "    w3 = tl.where(sign3, -mag3.to(tl.float16), mag3.to(tl.float16))\n",
    "    \n",
    "    return w0, w1, w2, w3\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    bits_ptr,\n",
    "    activations_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def matmul_triton(bits, activations):\n",
    "    B, N, M = activations.shape\n",
    "    output = torch.empty_like(activations)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']), )\n",
    "    matmul_kernel[grid](bits, activations, output, N, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "N = 256\n",
    "bits = torch.randint(\n",
    "    0, \n",
    "    2**15 - 1, \n",
    "    (N, 64,), \n",
    "    dtype=torch.int16\n",
    ").cuda()\n",
    "activations = torch.randn(N, 16, 16, dtype=torch.float16).cuda()\n",
    "\n",
    "res = matmul_python(bits, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a621402",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def decode_fp4_triton(bits):\n",
    "    \"\"\"Decodes 16-bit value into 4 FP4 values\"\"\"\n",
    "    val0 = (bits >> 12) & 0xF\n",
    "    val1 = (bits >> 8) & 0xF\n",
    "    val2 = (bits >> 4) & 0xF\n",
    "    val3 = bits & 0xF\n",
    "    \n",
    "    sign0 = (val0 & 0x8) != 0\n",
    "    sign1 = (val1 & 0x8) != 0\n",
    "    sign2 = (val2 & 0x8) != 0\n",
    "    sign3 = (val3 & 0x8) != 0\n",
    "    \n",
    "    mag0 = (val0 & 0x7)\n",
    "    mag1 = (val1 & 0x7)\n",
    "    mag2 = (val2 & 0x7)\n",
    "    mag3 = (val3 & 0x7)\n",
    "    \n",
    "    w0 = tl.where(sign0, -mag0, mag0)\n",
    "    w1 = tl.where(sign1, -mag1, mag1)\n",
    "    w2 = tl.where(sign2, -mag2, mag2)\n",
    "    w3 = tl.where(sign3, -mag3, mag3)\n",
    "    \n",
    "    return w0, w1, w2, w3\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    bits_ptr,\n",
    "    activations_ptr,\n",
    "    output_ptr,\n",
    "    N,  # Number of blocks in weight matrix\n",
    "    M,  # Activation dimension\n",
    "    B,  # Batch size\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # 2D grid: batch_id x block_id\n",
    "    batch_id = tl.program_id(axis=0)\n",
    "    block_id = tl.program_id(axis=1)\n",
    "    \n",
    "    # Each block processes 16x16 submatrix\n",
    "    block_start = block_id * 16\n",
    "    row_offsets = tl.arange(0, 16)\n",
    "    col_offsets = tl.arange(0, 16)\n",
    "    \n",
    "    # Load the bits for this block\n",
    "    bits_offset = batch_id * N + block_id\n",
    "    bits = tl.load(bits_ptr + bits_offset)\n",
    "    \n",
    "    # Decode FP4 values\n",
    "    w0, w1, w2, w3 = decode_fp4_triton(bits)\n",
    "    \n",
    "    # Create 16x16 weight matrix from decoded values\n",
    "    # We need to properly arrange the 4 values into a 4x4 pattern repeated 4 times\n",
    "    # First, create a 4x4 block from the 4 values\n",
    "    weight_block = tl.zeros((16, 16), dtype=tl.float16)\n",
    "    \n",
    "    # Fill the diagonal blocks with the decoded values\n",
    "    # This creates a block diagonal matrix where each 4x4 block has the same value\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            row = i * 4 + j\n",
    "            col = i * 4 + j\n",
    "            if i == 0:\n",
    "                weight_block = tl.where((row_offsets[:, None] == row) & (col_offsets[None, :] == col), \n",
    "                                      w0, weight_block)\n",
    "            elif i == 1:\n",
    "                weight_block = tl.where((row_offsets[:, None] == row) & (col_offsets[None, :] == col), \n",
    "                                      w1, weight_block)\n",
    "            elif i == 2:\n",
    "                weight_block = tl.where((row_offsets[:, None] == row) & (col_offsets[None, :] == col), \n",
    "                                      w2, weight_block)\n",
    "            else:\n",
    "                weight_block = tl.where((row_offsets[:, None] == row) & (col_offsets[None, :] == col), \n",
    "                                      w3, weight_block)\n",
    "    \n",
    "    # Load activations for this block (16 elements)\n",
    "    act_offsets = batch_id * M + block_start + row_offsets\n",
    "    activations = tl.load(activations_ptr + act_offsets, mask=row_offsets < M)\n",
    "    \n",
    "    # Perform the matrix multiplication for this block\n",
    "    # activations: [16], weight_block: [16, 16] -> result: [16]\n",
    "    result = tl.dot(activations, weight_block)\n",
    "    \n",
    "    # Store the result\n",
    "    output_offsets = batch_id * M + block_start + col_offsets\n",
    "    tl.store(output_ptr + output_offsets, result, mask=col_offsets < M)\n",
    "\n",
    "def matmul_triton(bits, activations):\n",
    "    B, N, M = activations.shape\n",
    "    # Reshape bits to match the expected format: [B, N//16]\n",
    "    bits_reshaped = bits.reshape(B, -1)\n",
    "    \n",
    "    output = torch.zeros_like(activations)\n",
    "    \n",
    "    # Grid: batch_size x number_of_blocks\n",
    "    grid = lambda meta: (B, triton.cdiv(M, 16))\n",
    "    \n",
    "    matmul_kernel[grid](\n",
    "        bits_reshaped, \n",
    "        activations, \n",
    "        output, \n",
    "        bits_reshaped.shape[1],  # N_blocks\n",
    "        M,  # M dimension\n",
    "        B,  # Batch size\n",
    "        BLOCK_SIZE=16\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4200f5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterpreterError",
     "evalue": "AssertionError(\"Both inputs must be either 2D or 3D; (lhs: ['constexpr[16]'] vs rhs: ['constexpr[16]', 'constexpr[16]'])\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1275\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1274\u001b[39m                 interpreter_builder.set_grid_idx(x, y, z)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mmatmul_kernel\u001b[39m\u001b[34m(bits_ptr, activations_ptr, output_ptr, N, M, B, BLOCK_SIZE)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Perform the matrix multiplication for this block\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# activations: [16], weight_block: [16, 16] -> result: [16]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m result = \u001b[43mtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Store the result\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:781\u001b[39m, in \u001b[36m_patch_attr.<locals>.<lambda>\u001b[39m\u001b[34m(member, *args, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m semantic = TritonSemantic(builder)\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m new_member = \u001b[38;5;28;01mlambda\u001b[39;00m *args, member=member, **kwargs: (\u001b[43mmember\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                                                             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                                                             \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_semantic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_semantic\u001b[49m\u001b[43m=\u001b[49m\u001b[43msemantic\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28msetattr\u001b[39m(obj, name, new_member)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:42\u001b[39m, in \u001b[36mbuiltin.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDid you forget to add @triton.jit ? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m(`_semantic` argument must be provided outside of JIT functions.)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/core.py:2045\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(input, other, acc, input_precision, allow_tf32, max_num_imprecise_acc, out_dtype, _semantic)\u001b[39m\n\u001b[32m   2044\u001b[39m acc = _unwrap_if_constexpr(acc)\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_semantic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_imprecise_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/language/semantic.py:1497\u001b[39m, in \u001b[36mTritonSemantic.dot\u001b[39m\u001b[34m(self, lhs, rhs, acc, input_precision, max_num_imprecise_acc, out_dtype)\u001b[39m\n\u001b[32m   1496\u001b[39m rhs_rank = \u001b[38;5;28mlen\u001b[39m(rhs.shape)\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m lhs_rank == rhs_rank == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m lhs_rank == rhs_rank == \u001b[32m3\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBoth inputs must be either 2D or 3D; (lhs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlhs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs rhs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrhs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m lhs.shape[-\u001b[32m1\u001b[39m].value == rhs.shape[\n\u001b[32m   1499\u001b[39m     -\u001b[32m2\u001b[39m].value, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst input shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlhs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) and second input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrhs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not compatible for matmul (second index of first shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlhs.shape[-\u001b[32m1\u001b[39m].value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be equal to first index of second shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrhs.shape[-\u001b[32m2\u001b[39m].value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Both inputs must be either 2D or 3D; (lhs: ['constexpr[16]'] vs rhs: ['constexpr[16]', 'constexpr[16]'])",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInterpreterError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res_triton = \u001b[43mmatmul_triton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mmatmul_triton\u001b[39m\u001b[34m(bits, activations)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Grid: batch_size x number_of_blocks\u001b[39;00m\n\u001b[32m     96\u001b[39m grid = \u001b[38;5;28;01mlambda\u001b[39;00m meta: (B, triton.cdiv(M, \u001b[32m16\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mmatmul_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbits_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbits_reshaped\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# N_blocks\u001b[39;49;00m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# M dimension\u001b[39;49;00m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/triton_env/lib/python3.12/site-packages/triton/runtime/interpreter.py:1279\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m triton.knobs.compilation.front_end_debugging:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InterpreterError(\u001b[38;5;28mrepr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1280\u001b[39m \u001b[38;5;66;03m# copy arguments back to propagate side-effects\u001b[39;00m\n\u001b[32m   1281\u001b[39m \u001b[38;5;28mself\u001b[39m._restore_args_dev(args_dev, args_hst, kwargs, kwargs_hst)\n",
      "\u001b[31mInterpreterError\u001b[39m: AssertionError(\"Both inputs must be either 2D or 3D; (lhs: ['constexpr[16]'] vs rhs: ['constexpr[16]', 'constexpr[16]'])\")"
     ]
    }
   ],
   "source": [
    "res_triton = matmul_triton(bits, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9af5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
